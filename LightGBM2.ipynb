{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jess\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Jess\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pickle\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import time, datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import gc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import jieba, pdb\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "# load stopwords set\n",
    "stopword_set = set()\n",
    "with open('jieba_dict/stopwords.txt','r', encoding='utf-8') as stopwords:\n",
    "    for stopword in stopwords:\n",
    "        stopword_set.add(stopword.strip('\\n'))\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"word2vec2.model\")\n",
    "\n",
    "\n",
    "def create_dictionaries(p_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(p_model.wv.vocab.keys(), allow_update=True)\n",
    "    w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2indx.keys()}  # 词语的词向量\n",
    "    return w2indx, w2vec\n",
    "\n",
    "def Convert_orderid(x):\n",
    "    return str(x).strip('\\n')\n",
    "\n",
    "def Convert_Date(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = pd.to_datetime(Year+'-'+Month+'-'+Day)\n",
    "    return date1\n",
    "\n",
    "def Date2Ticks(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = str(Year+'/'+Month+'/'+Day)\n",
    "    return time.mktime(datetime.datetime.strptime(date1, \"%Y/%m/%d\").timetuple())\n",
    "\n",
    "index_dict, word_vectors= create_dictionaries(model)\n",
    "output = open(\"wordwmbedding.pkl\", 'wb')\n",
    "pickle.dump(index_dict, output)  # 索引字典\n",
    "pickle.dump(word_vectors, output)  # 词向量字典\n",
    "output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 296237)\n"
     ]
    }
   ],
   "source": [
    "df_train_1 = pd.read_csv(\"myTrain.csv\")\n",
    "df_result_1 = pd.read_csv(\"myTest.csv\")\n",
    "\n",
    "\n",
    "used_columns=['order_id','tick_diff', 'Source_1', 'Source_2', 'Unit', 'deal_or_not',\n",
    "'people_amount', 'Begin_Tick','days', 'Order_Tick', 'Area', 'SubLine', 'price','PreDays','Begin_Date_Weekday', \n",
    "'Order_Date_Weekday', 'Return_Date_Weekday', 'fly_t', 'fly_date',\n",
    "\"src_airport\", \"arrive_t\", \"arrive_date\", \"dst_airport\",'pred0','pred1','pred2','pred3','pred4','pred5','pred6', 'pred7','product_name']\n",
    "\n",
    "df_train_1 = df_train_1[used_columns]\n",
    "df_result_1 = df_result_1[used_columns]\n",
    "\n",
    "myPred = []\n",
    "for i in range(8):\n",
    "    myPred.append(df_train_1['pred%d'%(i)].values.tolist())\n",
    "myPred = np.asarray(myPred)\n",
    "print(myPred.shape)\n",
    "predFeat = []\n",
    "for i in range(len(myPred[0,:])):\n",
    "    predFeat.append([np.max(myPred[:,i]), np.percentile(myPred[:,i], 75), \n",
    "                     np.median(myPred[:,i]), np.percentile(myPred[:,i],25),np.min(myPred[:,i])]\n",
    "                   )\n",
    "train_feat=np.asarray(predFeat)\n",
    "\n",
    "myPred = []\n",
    "for i in range(8):\n",
    "    myPred.append(df_result_1['pred%d'%(i)].values.tolist())\n",
    "myPred = np.asarray(myPred)\n",
    "predFeat = []\n",
    "for i in range(len(myPred[0,:])):\n",
    "    predFeat.append([np.max(myPred[:,i]), np.percentile(myPred[:,i], 75), \n",
    "                     np.median(myPred[:,i]), np.percentile(myPred[:,i],25),\n",
    "                   np.min(myPred[:,i])])\n",
    "test_feat=np.asarray(predFeat)\n",
    "\n",
    "for i in range(8):\n",
    "    del df_result_1['pred%d'%(i)]\n",
    "    del df_train_1['pred%d'%(i)]\n",
    "\n",
    "Y=df_train_1['deal_or_not'].values.tolist()\n",
    "swX = (df_train_1['product_name']).values.tolist()\n",
    "del df_train_1['deal_or_not'] \n",
    "del df_train_1['product_name']\n",
    "del df_train_1['order_id']\n",
    "X = df_train_1.values.tolist()\n",
    "\n",
    "swrx = (df_result_1['product_name']).values.tolist()\n",
    "rid = df_result_1['order_id'].values.tolist()\n",
    "del df_result_1['product_name']\n",
    "del df_result_1['deal_or_not'] \n",
    "del df_result_1['order_id'] \n",
    "rx = df_result_1.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sX, Y =np.asarray(X), np.asarray(Y)\n",
    "X=np.concatenate([sX, train_feat], axis=1)\n",
    "rx = np.asarray(rx)\n",
    "rx=np.concatenate([rx, test_feat], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def text_to_index_array(p_new_dic, p_sen):  # 文本转为索引数字模式\n",
    "    new_sentences = []\n",
    "    for sen in p_sen:\n",
    "        new_sen = []\n",
    "        for word in str(sen):\n",
    "            try:\n",
    "                new_sen.append(p_new_dic[word])  # 单词转索引数字\n",
    "            except:\n",
    "                new_sen.append(0)  # 索引字典里没有的词转为数字0\n",
    "        new_sentences.append(new_sen)\n",
    "\n",
    "    return np.array(new_sentences)\n",
    "\n",
    "\n",
    "wX = text_to_index_array(index_dict, swX)\n",
    "wrx = text_to_index_array(index_dict, swrx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296237, 86)\n",
      "(99736, 86)\n",
      "[8046. 8801.    0.    0.]\n"
     ]
    }
   ],
   "source": [
    "wX = sequence.pad_sequences(wX, maxlen=60)\n",
    "wrx = sequence.pad_sequences(wrx, maxlen=60)\n",
    "\n",
    "\n",
    "X=np.concatenate([X, wX], axis=1)\n",
    "rx=np.concatenate([rx, wrx], axis=1)\n",
    "# xlen=len(X)\n",
    "# from sklearn.preprocessing import normalize\n",
    "# Xtmp=normalize(np.concatenate([X, rx], axis=0),norm='max', axis=0)\n",
    "# X=Xtmp[:xlen]\n",
    "# rx=Xtmp[xlen:]\n",
    "\n",
    "print(X.shape)\n",
    "print(rx.shape)\n",
    "\n",
    "print(X[5,-5:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Index: [     0      1      2 ... 296233 296234 296236] ,Val Index: [    15     37     46 ... 296213 296215 296235]\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\ttraining's auc: 0.696865\tvalid_1's auc: 0.686925\n",
      "[100]\ttraining's auc: 0.698541\tvalid_1's auc: 0.687842\n",
      "[150]\ttraining's auc: 0.700427\tvalid_1's auc: 0.688887\n",
      "[200]\ttraining's auc: 0.702167\tvalid_1's auc: 0.689428\n",
      "[250]\ttraining's auc: 0.704289\tvalid_1's auc: 0.690127\n",
      "[300]\ttraining's auc: 0.706134\tvalid_1's auc: 0.690623\n",
      "[350]\ttraining's auc: 0.708071\tvalid_1's auc: 0.690983\n",
      "[400]\ttraining's auc: 0.710481\tvalid_1's auc: 0.691505\n",
      "[450]\ttraining's auc: 0.71275\tvalid_1's auc: 0.691899\n",
      "[500]\ttraining's auc: 0.714956\tvalid_1's auc: 0.692295\n",
      "[550]\ttraining's auc: 0.716929\tvalid_1's auc: 0.692714\n",
      "[600]\ttraining's auc: 0.719201\tvalid_1's auc: 0.693164\n",
      "[650]\ttraining's auc: 0.721466\tvalid_1's auc: 0.693517\n",
      "[700]\ttraining's auc: 0.723846\tvalid_1's auc: 0.693921\n",
      "[750]\ttraining's auc: 0.726028\tvalid_1's auc: 0.694246\n",
      "[800]\ttraining's auc: 0.727767\tvalid_1's auc: 0.69451\n",
      "[850]\ttraining's auc: 0.729922\tvalid_1's auc: 0.694923\n",
      "[900]\ttraining's auc: 0.732219\tvalid_1's auc: 0.695217\n",
      "[950]\ttraining's auc: 0.734012\tvalid_1's auc: 0.695557\n",
      "[1000]\ttraining's auc: 0.735755\tvalid_1's auc: 0.695883\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.735755\tvalid_1's auc: 0.695883\n",
      "Fold  1 AUC : 0.695883\n",
      "Train Index: [     0      1      2 ... 296233 296235 296236] ,Val Index: [     4      6     20 ... 296214 296220 296234]\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\ttraining's auc: 0.696135\tvalid_1's auc: 0.694359\n",
      "[100]\ttraining's auc: 0.697985\tvalid_1's auc: 0.69525\n",
      "[150]\ttraining's auc: 0.699767\tvalid_1's auc: 0.695566\n",
      "[200]\ttraining's auc: 0.701646\tvalid_1's auc: 0.696291\n",
      "[250]\ttraining's auc: 0.70361\tvalid_1's auc: 0.696662\n",
      "[300]\ttraining's auc: 0.705484\tvalid_1's auc: 0.697128\n",
      "[350]\ttraining's auc: 0.707419\tvalid_1's auc: 0.697393\n",
      "[400]\ttraining's auc: 0.709817\tvalid_1's auc: 0.698032\n",
      "[450]\ttraining's auc: 0.71227\tvalid_1's auc: 0.698641\n",
      "[500]\ttraining's auc: 0.714403\tvalid_1's auc: 0.699119\n",
      "[550]\ttraining's auc: 0.716467\tvalid_1's auc: 0.69967\n",
      "[600]\ttraining's auc: 0.718609\tvalid_1's auc: 0.700068\n",
      "[650]\ttraining's auc: 0.720921\tvalid_1's auc: 0.700701\n",
      "[700]\ttraining's auc: 0.72344\tvalid_1's auc: 0.701218\n",
      "[750]\ttraining's auc: 0.725883\tvalid_1's auc: 0.7017\n",
      "[800]\ttraining's auc: 0.727538\tvalid_1's auc: 0.70217\n",
      "[850]\ttraining's auc: 0.729609\tvalid_1's auc: 0.702538\n",
      "[900]\ttraining's auc: 0.731798\tvalid_1's auc: 0.702859\n",
      "[950]\ttraining's auc: 0.73379\tvalid_1's auc: 0.703208\n",
      "[1000]\ttraining's auc: 0.735451\tvalid_1's auc: 0.703382\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.735451\tvalid_1's auc: 0.703382\n",
      "Fold  2 AUC : 0.703382\n",
      "Train Index: [     0      1      2 ... 296234 296235 296236] ,Val Index: [    19     26     31 ... 296197 296229 296230]\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\ttraining's auc: 0.697284\tvalid_1's auc: 0.680643\n",
      "[100]\ttraining's auc: 0.699171\tvalid_1's auc: 0.681512\n",
      "[150]\ttraining's auc: 0.701245\tvalid_1's auc: 0.682228\n",
      "[200]\ttraining's auc: 0.703071\tvalid_1's auc: 0.682466\n",
      "[250]\ttraining's auc: 0.705199\tvalid_1's auc: 0.682966\n",
      "[300]\ttraining's auc: 0.706988\tvalid_1's auc: 0.683344\n",
      "[350]\ttraining's auc: 0.708916\tvalid_1's auc: 0.683634\n",
      "[400]\ttraining's auc: 0.711351\tvalid_1's auc: 0.68421\n",
      "[450]\ttraining's auc: 0.713755\tvalid_1's auc: 0.684597\n",
      "[500]\ttraining's auc: 0.71599\tvalid_1's auc: 0.68496\n",
      "[550]\ttraining's auc: 0.717992\tvalid_1's auc: 0.685265\n",
      "[600]\ttraining's auc: 0.720186\tvalid_1's auc: 0.685587\n",
      "[650]\ttraining's auc: 0.722128\tvalid_1's auc: 0.685871\n",
      "[700]\ttraining's auc: 0.724455\tvalid_1's auc: 0.686293\n",
      "[750]\ttraining's auc: 0.726782\tvalid_1's auc: 0.686822\n",
      "[800]\ttraining's auc: 0.728357\tvalid_1's auc: 0.687002\n",
      "[850]\ttraining's auc: 0.730415\tvalid_1's auc: 0.687332\n",
      "[900]\ttraining's auc: 0.732734\tvalid_1's auc: 0.68766\n",
      "[950]\ttraining's auc: 0.734726\tvalid_1's auc: 0.68785\n",
      "[1000]\ttraining's auc: 0.736493\tvalid_1's auc: 0.688091\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.736493\tvalid_1's auc: 0.688091\n",
      "Fold  3 AUC : 0.688091\n",
      "Train Index: [     0      1      2 ... 296234 296235 296236] ,Val Index: [     5     10     11 ... 296208 296221 296227]\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\ttraining's auc: 0.695363\tvalid_1's auc: 0.691459\n",
      "[100]\ttraining's auc: 0.697496\tvalid_1's auc: 0.692638\n",
      "[150]\ttraining's auc: 0.699558\tvalid_1's auc: 0.69397\n",
      "[200]\ttraining's auc: 0.701623\tvalid_1's auc: 0.694618\n",
      "[250]\ttraining's auc: 0.703633\tvalid_1's auc: 0.695176\n",
      "[300]\ttraining's auc: 0.705609\tvalid_1's auc: 0.695671\n",
      "[350]\ttraining's auc: 0.707629\tvalid_1's auc: 0.696132\n",
      "[400]\ttraining's auc: 0.710045\tvalid_1's auc: 0.696832\n",
      "[450]\ttraining's auc: 0.712584\tvalid_1's auc: 0.697307\n",
      "[500]\ttraining's auc: 0.71467\tvalid_1's auc: 0.697956\n",
      "[550]\ttraining's auc: 0.716743\tvalid_1's auc: 0.698483\n",
      "[600]\ttraining's auc: 0.719174\tvalid_1's auc: 0.69929\n",
      "[650]\ttraining's auc: 0.721289\tvalid_1's auc: 0.699619\n",
      "[700]\ttraining's auc: 0.72389\tvalid_1's auc: 0.700215\n",
      "[750]\ttraining's auc: 0.726138\tvalid_1's auc: 0.700693\n",
      "[800]\ttraining's auc: 0.727812\tvalid_1's auc: 0.701039\n",
      "[850]\ttraining's auc: 0.729865\tvalid_1's auc: 0.701412\n",
      "[900]\ttraining's auc: 0.731967\tvalid_1's auc: 0.70173\n",
      "[950]\ttraining's auc: 0.733939\tvalid_1's auc: 0.702063\n",
      "[1000]\ttraining's auc: 0.735638\tvalid_1's auc: 0.702396\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.735638\tvalid_1's auc: 0.702396\n",
      "Fold  4 AUC : 0.702396\n",
      "Train Index: [     0      1      3 ... 296234 296235 296236] ,Val Index: [     2      9     52 ... 296225 296228 296232]\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\ttraining's auc: 0.695541\tvalid_1's auc: 0.694297\n",
      "[100]\ttraining's auc: 0.697272\tvalid_1's auc: 0.695862\n",
      "[150]\ttraining's auc: 0.699209\tvalid_1's auc: 0.696832\n",
      "[200]\ttraining's auc: 0.701152\tvalid_1's auc: 0.697203\n",
      "[250]\ttraining's auc: 0.7032\tvalid_1's auc: 0.697884\n",
      "[300]\ttraining's auc: 0.705049\tvalid_1's auc: 0.698308\n",
      "[350]\ttraining's auc: 0.707063\tvalid_1's auc: 0.698766\n",
      "[400]\ttraining's auc: 0.709442\tvalid_1's auc: 0.699222\n",
      "[450]\ttraining's auc: 0.711866\tvalid_1's auc: 0.699675\n",
      "[500]\ttraining's auc: 0.714159\tvalid_1's auc: 0.700236\n",
      "[550]\ttraining's auc: 0.716036\tvalid_1's auc: 0.700679\n",
      "[600]\ttraining's auc: 0.718271\tvalid_1's auc: 0.701292\n",
      "[650]\ttraining's auc: 0.72067\tvalid_1's auc: 0.701812\n",
      "[700]\ttraining's auc: 0.723072\tvalid_1's auc: 0.702326\n",
      "[750]\ttraining's auc: 0.725432\tvalid_1's auc: 0.702611\n",
      "[800]\ttraining's auc: 0.727159\tvalid_1's auc: 0.702794\n",
      "[850]\ttraining's auc: 0.729251\tvalid_1's auc: 0.703121\n",
      "[900]\ttraining's auc: 0.731508\tvalid_1's auc: 0.703462\n",
      "[950]\ttraining's auc: 0.733614\tvalid_1's auc: 0.703861\n",
      "[1000]\ttraining's auc: 0.735386\tvalid_1's auc: 0.704118\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.735386\tvalid_1's auc: 0.704118\n",
      "Fold  5 AUC : 0.704118\n",
      "Train Index: [     0      1      2 ... 296233 296234 296235] ,Val Index: [    12     14     32 ... 296226 296231 296236]\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\ttraining's auc: 0.695733\tvalid_1's auc: 0.694772\n",
      "[100]\ttraining's auc: 0.697505\tvalid_1's auc: 0.695691\n",
      "[150]\ttraining's auc: 0.6993\tvalid_1's auc: 0.696259\n",
      "[200]\ttraining's auc: 0.701434\tvalid_1's auc: 0.697012\n",
      "[250]\ttraining's auc: 0.703353\tvalid_1's auc: 0.697462\n",
      "[300]\ttraining's auc: 0.705327\tvalid_1's auc: 0.69801\n",
      "[350]\ttraining's auc: 0.70744\tvalid_1's auc: 0.698332\n",
      "[400]\ttraining's auc: 0.709897\tvalid_1's auc: 0.698672\n",
      "[450]\ttraining's auc: 0.712286\tvalid_1's auc: 0.699009\n",
      "[500]\ttraining's auc: 0.714287\tvalid_1's auc: 0.699491\n",
      "[550]\ttraining's auc: 0.716369\tvalid_1's auc: 0.699761\n",
      "[600]\ttraining's auc: 0.718583\tvalid_1's auc: 0.700385\n",
      "[650]\ttraining's auc: 0.720795\tvalid_1's auc: 0.700832\n",
      "[700]\ttraining's auc: 0.723217\tvalid_1's auc: 0.701184\n",
      "[750]\ttraining's auc: 0.725296\tvalid_1's auc: 0.701514\n",
      "[800]\ttraining's auc: 0.726975\tvalid_1's auc: 0.701732\n",
      "[850]\ttraining's auc: 0.729218\tvalid_1's auc: 0.702007\n",
      "[900]\ttraining's auc: 0.731407\tvalid_1's auc: 0.702328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[950]\ttraining's auc: 0.733355\tvalid_1's auc: 0.702567\n",
      "[1000]\ttraining's auc: 0.735118\tvalid_1's auc: 0.702902\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.735118\tvalid_1's auc: 0.702902\n",
      "Fold  6 AUC : 0.702902\n",
      "Train Index: [     0      1      2 ... 296234 296235 296236] ,Val Index: [    13     21     22 ... 296205 296209 296233]\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\ttraining's auc: 0.695844\tvalid_1's auc: 0.691959\n",
      "[100]\ttraining's auc: 0.697711\tvalid_1's auc: 0.692837\n",
      "[150]\ttraining's auc: 0.699616\tvalid_1's auc: 0.693743\n",
      "[200]\ttraining's auc: 0.701456\tvalid_1's auc: 0.694289\n",
      "[250]\ttraining's auc: 0.703538\tvalid_1's auc: 0.695114\n",
      "[300]\ttraining's auc: 0.705529\tvalid_1's auc: 0.695665\n",
      "[350]\ttraining's auc: 0.707607\tvalid_1's auc: 0.696331\n",
      "[400]\ttraining's auc: 0.710187\tvalid_1's auc: 0.696921\n",
      "[450]\ttraining's auc: 0.712645\tvalid_1's auc: 0.69768\n",
      "[500]\ttraining's auc: 0.714671\tvalid_1's auc: 0.698025\n",
      "[550]\ttraining's auc: 0.716668\tvalid_1's auc: 0.69852\n",
      "[600]\ttraining's auc: 0.718941\tvalid_1's auc: 0.699018\n",
      "[650]\ttraining's auc: 0.721214\tvalid_1's auc: 0.699411\n",
      "[700]\ttraining's auc: 0.723644\tvalid_1's auc: 0.699807\n",
      "[750]\ttraining's auc: 0.725845\tvalid_1's auc: 0.700163\n",
      "[800]\ttraining's auc: 0.727527\tvalid_1's auc: 0.700441\n",
      "[850]\ttraining's auc: 0.729825\tvalid_1's auc: 0.700791\n",
      "[900]\ttraining's auc: 0.731977\tvalid_1's auc: 0.700982\n",
      "[950]\ttraining's auc: 0.73379\tvalid_1's auc: 0.701264\n",
      "[1000]\ttraining's auc: 0.735368\tvalid_1's auc: 0.701499\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.735368\tvalid_1's auc: 0.701499\n",
      "Fold  7 AUC : 0.701499\n",
      "Train Index: [     0      2      3 ... 296234 296235 296236] ,Val Index: [     1     16     25 ... 296155 296206 296216]\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\ttraining's auc: 0.696074\tvalid_1's auc: 0.692061\n",
      "[100]\ttraining's auc: 0.697914\tvalid_1's auc: 0.692891\n",
      "[150]\ttraining's auc: 0.699795\tvalid_1's auc: 0.693602\n",
      "[200]\ttraining's auc: 0.701593\tvalid_1's auc: 0.694415\n",
      "[250]\ttraining's auc: 0.70365\tvalid_1's auc: 0.6951\n",
      "[300]\ttraining's auc: 0.705639\tvalid_1's auc: 0.695637\n",
      "[350]\ttraining's auc: 0.707556\tvalid_1's auc: 0.696083\n",
      "[400]\ttraining's auc: 0.710294\tvalid_1's auc: 0.696497\n",
      "[450]\ttraining's auc: 0.712641\tvalid_1's auc: 0.69708\n",
      "[500]\ttraining's auc: 0.714678\tvalid_1's auc: 0.697422\n",
      "[550]\ttraining's auc: 0.716766\tvalid_1's auc: 0.697837\n",
      "[600]\ttraining's auc: 0.719016\tvalid_1's auc: 0.698375\n",
      "[650]\ttraining's auc: 0.721187\tvalid_1's auc: 0.698802\n",
      "[700]\ttraining's auc: 0.723585\tvalid_1's auc: 0.699228\n",
      "[750]\ttraining's auc: 0.725944\tvalid_1's auc: 0.699757\n",
      "[800]\ttraining's auc: 0.727642\tvalid_1's auc: 0.700054\n",
      "[850]\ttraining's auc: 0.729868\tvalid_1's auc: 0.700532\n",
      "[900]\ttraining's auc: 0.732041\tvalid_1's auc: 0.700941\n",
      "[950]\ttraining's auc: 0.734027\tvalid_1's auc: 0.701131\n",
      "[1000]\ttraining's auc: 0.735687\tvalid_1's auc: 0.70127\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.735687\tvalid_1's auc: 0.70127\n",
      "Fold  8 AUC : 0.701270\n",
      "Train Index: [     0      1      2 ... 296234 296235 296236] ,Val Index: [     7      8     43 ... 296210 296212 296224]\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\ttraining's auc: 0.696258\tvalid_1's auc: 0.689016\n",
      "[100]\ttraining's auc: 0.698174\tvalid_1's auc: 0.690342\n",
      "[150]\ttraining's auc: 0.699961\tvalid_1's auc: 0.690887\n",
      "[200]\ttraining's auc: 0.70202\tvalid_1's auc: 0.691391\n",
      "[250]\ttraining's auc: 0.703889\tvalid_1's auc: 0.691851\n",
      "[300]\ttraining's auc: 0.705874\tvalid_1's auc: 0.692194\n",
      "[350]\ttraining's auc: 0.707997\tvalid_1's auc: 0.692667\n",
      "[400]\ttraining's auc: 0.710464\tvalid_1's auc: 0.69316\n",
      "[450]\ttraining's auc: 0.712773\tvalid_1's auc: 0.693671\n",
      "[500]\ttraining's auc: 0.714732\tvalid_1's auc: 0.693969\n",
      "[550]\ttraining's auc: 0.717071\tvalid_1's auc: 0.694362\n",
      "[600]\ttraining's auc: 0.719389\tvalid_1's auc: 0.695005\n",
      "[650]\ttraining's auc: 0.721522\tvalid_1's auc: 0.695536\n",
      "[700]\ttraining's auc: 0.72406\tvalid_1's auc: 0.696002\n",
      "[750]\ttraining's auc: 0.726229\tvalid_1's auc: 0.696315\n",
      "[800]\ttraining's auc: 0.727946\tvalid_1's auc: 0.696501\n",
      "[850]\ttraining's auc: 0.730181\tvalid_1's auc: 0.696694\n",
      "[900]\ttraining's auc: 0.732347\tvalid_1's auc: 0.696883\n",
      "[950]\ttraining's auc: 0.734075\tvalid_1's auc: 0.697148\n",
      "[1000]\ttraining's auc: 0.735743\tvalid_1's auc: 0.697297\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.735743\tvalid_1's auc: 0.697297\n",
      "Fold  9 AUC : 0.697297\n",
      "Train Index: [     1      2      4 ... 296234 296235 296236] ,Val Index: [     0      3     17 ... 296172 296200 296204]\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\ttraining's auc: 0.6966\tvalid_1's auc: 0.684129\n",
      "[100]\ttraining's auc: 0.698366\tvalid_1's auc: 0.68517\n",
      "[150]\ttraining's auc: 0.700109\tvalid_1's auc: 0.685939\n",
      "[200]\ttraining's auc: 0.702145\tvalid_1's auc: 0.686449\n",
      "[250]\ttraining's auc: 0.704042\tvalid_1's auc: 0.687086\n",
      "[300]\ttraining's auc: 0.705991\tvalid_1's auc: 0.687459\n",
      "[350]\ttraining's auc: 0.707954\tvalid_1's auc: 0.68798\n",
      "[400]\ttraining's auc: 0.710308\tvalid_1's auc: 0.688448\n",
      "[450]\ttraining's auc: 0.712638\tvalid_1's auc: 0.689043\n",
      "[500]\ttraining's auc: 0.71473\tvalid_1's auc: 0.689529\n",
      "[550]\ttraining's auc: 0.716728\tvalid_1's auc: 0.690136\n",
      "[600]\ttraining's auc: 0.718983\tvalid_1's auc: 0.690667\n",
      "[650]\ttraining's auc: 0.721166\tvalid_1's auc: 0.691361\n",
      "[700]\ttraining's auc: 0.723547\tvalid_1's auc: 0.691812\n",
      "[750]\ttraining's auc: 0.725716\tvalid_1's auc: 0.692261\n",
      "[800]\ttraining's auc: 0.727459\tvalid_1's auc: 0.692567\n",
      "[850]\ttraining's auc: 0.729722\tvalid_1's auc: 0.69289\n",
      "[900]\ttraining's auc: 0.731922\tvalid_1's auc: 0.69313\n",
      "[950]\ttraining's auc: 0.733954\tvalid_1's auc: 0.693476\n",
      "[1000]\ttraining's auc: 0.735689\tvalid_1's auc: 0.69368\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's auc: 0.735689\tvalid_1's auc: 0.69368\n",
      "Fold 10 AUC : 0.693680\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds = StratifiedKFold(n_splits= 10, shuffle=True, random_state=666)\n",
    "\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "sub_preds = np.zeros(rx.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, Y)):\n",
    "\n",
    "    train_x, train_y = X[train_idx,:], Y[train_idx]\n",
    "    valid_x, valid_y = X[valid_idx,:], Y[valid_idx]\n",
    "\n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    params = {\n",
    "    'nthread': 8, 'boosting_type': 'dart','objective': 'regression', 'metric': 'auc', \n",
    "        'learning_rate': 0.01, 'num_leaves': 70,\n",
    "    'max_depth': 9, 'subsample': 0.7, 'feature_fraction': 0.7, \n",
    "        'min_split_gain': 0.09, 'min_child_weight': 9.5,\n",
    "        'min_data_in_leaf':10,\n",
    "        'bagging_freq':5,\n",
    "    'drop_rate':0.5, 'skip_drop':0.5, 'max_drop':5, 'uniform_drop':False, \n",
    "        'xgboost_dart_mode':True, 'drop_seed':1 }\n",
    "\n",
    "\n",
    "    if n_fold >= 0:\n",
    "        dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "        dval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain)\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=1000, valid_sets=[dtrain,dval], early_stopping_rounds=200, verbose_eval=50)\n",
    "\n",
    "\n",
    "        tmp_valid = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "        oof_preds[valid_idx] = tmp_valid\n",
    "        sub_preds += bst.predict(rx, num_iteration=bst.best_iteration) / folds.n_splits\n",
    "        \n",
    "\n",
    "        # Make the feature importance dataframe\n",
    "        gain = bst.feature_importance('gain')\n",
    "        fold_importance_df = pd.DataFrame({'feature':bst.feature_name(),'split':bst.feature_importance('split'),\n",
    "                                           'gain':100*gain/gain.sum(),  'fold':n_fold,}).sort_values('gain',ascending=False)\n",
    "\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "\n",
    "        del bst, train_x, train_y, valid_x, valid_y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nanMapping(x):\n",
    "    if np.isnan(x):\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "app_test = pd.read_csv('testing-set.csv', usecols=['order_id'])\n",
    "predF=pd.DataFrame({\"order_id\":rid, \"deal_or_not\":sub_preds})\n",
    "preds = pd.DataFrame({\"order_id\":app_test['order_id']})\n",
    "preds=preds.merge(predF, on=\"order_id\",how=\"outer\")\n",
    "preds['deal_or_not'] = preds.deal_or_not.apply(lambda x: nanMapping(x))\n",
    "preds.to_csv(\"output/lgb_dart_\" + str(roc_auc_score(Y, oof_preds)) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge, RidgeCV, ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from catboost import Pool, CatBoostRegressor, cv\n",
    "\n",
    "\n",
    "len1 = len(Y)\n",
    "tind = np.zeros(len1, np.int)\n",
    "for i in range(len1):\n",
    "    tind[i]=i\n",
    "import random as rn\n",
    "rn.Random(4).shuffle(tind)\n",
    "print(train_x.shape)\n",
    "\n",
    "train_x, train_y = X[tind[1000:],:], Y[tind[1000:]]\n",
    "valid_x, valid_y = X[tind[:1000],:], Y[tind[:1000]]\n",
    "\n",
    "model_ridge = Ridge(alpha = 5)\n",
    "model_ridge.fit(train_x, train_y)\n",
    "\n",
    "model_lasso = Lasso(alpha = 0.005)\n",
    "model_lasso.fit(train_x, train_y)\n",
    "\n",
    "model_en = ElasticNet(alpha = 0.005)\n",
    "model_en.fit(train_x, train_y)\n",
    "\n",
    "model_gbr = GradientBoostingRegressor(n_estimators=1200, \n",
    "                                      learning_rate=0.05,\n",
    "                                      max_depth=4, \n",
    "                                      max_features='sqrt',\n",
    "                                      min_samples_leaf=15, \n",
    "                                      min_samples_split=10, \n",
    "                                      loss='huber',\n",
    "                                      random_state=5)\n",
    "model_gbr.fit(train_x, train_y)\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.2,\n",
    "                             learning_rate=0.06,\n",
    "                             max_depth=3,\n",
    "                             n_estimators=1150)\n",
    "model_xgb(train_x, train_y)\n",
    "\n",
    "\n",
    "params = {\n",
    "'nthread': 8, 'boosting_type': 'dart','objective': 'regression', 'metric': 'auc', \n",
    "'learning_rate': 0.01, 'num_leaves': 70,\n",
    "'max_depth': 9, 'subsample': 0.9, 'feature_fraction': 0.9, \n",
    "'min_split_gain': 0.09, 'min_child_weight': 9.5,\n",
    "'drop_rate':0.5, 'skip_drop':0.5, 'max_drop':5, 'uniform_drop':False, \n",
    "'xgboost_dart_mode':True, 'drop_seed':5 }\n",
    "\n",
    "dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "dval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain)\n",
    "bst = lgb.train(params, dtrain, num_boost_round=15000, valid_sets=[dval], early_stopping_rounds=2500, \n",
    "                verbose_eval=500)\n",
    "\n",
    "tmp_valid = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "sub_preds = bst.predict(rx, num_iteration=bst.best_iteration) \n",
    "\n",
    "# Make the feature importance dataframe\n",
    "print('AUC : %.6f' % (roc_auc_score(valid_y, tmp_valid)))\n",
    "def nanMapping(x):\n",
    "    if np.isnan(x):\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "app_test = pd.read_csv('testing-set.csv', usecols=['order_id'])\n",
    "predF=pd.DataFrame({\"order_id\":rid, \"deal_or_not\":sub_preds})\n",
    "preds = pd.DataFrame({\"order_id\":app_test['order_id']})\n",
    "preds=preds.merge(predF, on=\"order_id\",how=\"outer\")\n",
    "preds['deal_or_not'] = preds.deal_or_not.apply(lambda x: nanMapping(x))\n",
    "preds.to_csv(\"output/lgb_dart_\" + str(roc_auc_score(valid_y, tmp_valid)) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['deal_or_not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
