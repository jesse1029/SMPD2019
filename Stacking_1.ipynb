{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1217\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pickle\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import time, datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import gc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import jieba, pdb\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "# load stopwords set\n",
    "stopword_set = set()\n",
    "with open('jieba_dict/stopwords.txt','r', encoding='utf-8') as stopwords:\n",
    "    for stopword in stopwords:\n",
    "        stopword_set.add(stopword.strip('\\n'))\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"word2vec2.model\")\n",
    "\n",
    "\n",
    "def create_dictionaries(p_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(p_model.wv.vocab.keys(), allow_update=True)\n",
    "    w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2indx.keys()}  # 词语的词向量\n",
    "    return w2indx, w2vec\n",
    "\n",
    "def Convert_orderid(x):\n",
    "    return str(x).strip('\\n')\n",
    "\n",
    "def Convert_Date(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = pd.to_datetime(Year+'-'+Month+'-'+Day)\n",
    "    return date1\n",
    "\n",
    "def Date2Ticks(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = str(Year+'/'+Month+'/'+Day)\n",
    "    return time.mktime(datetime.datetime.strptime(date1, \"%Y/%m/%d\").timetuple())\n",
    "\n",
    "index_dict, word_vectors= create_dictionaries(model)\n",
    "output = open(\"wordwmbedding.pkl\", 'wb')\n",
    "pickle.dump(index_dict, output)  # 索引字典\n",
    "pickle.dump(word_vectors, output)  # 词向量字典\n",
    "output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1217\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df_order = pd.read_csv(\"dataset/order.csv\")\n",
    "df_group = pd.read_csv(\"dataset/group.csv\")\n",
    "df_airline = pd.read_csv(\"dataset/airline2.csv\")\n",
    "df_day_schedule = pd.read_csv(\"day_schedule_processed.txt\")\n",
    "df_train = pd.read_csv(\"training-set.csv\")\n",
    "df_result = pd.read_csv(\"testing-set.csv\")\n",
    "# date Conversion\n",
    "\n",
    "month = {'Jan': '01', 'Feb': '02' , 'Mar':'03' ,'Apr': '04', \n",
    "'May': '05', 'Jun': '06' , 'Jul': '07' , 'Aug':'08', \n",
    "'Sep':'09', 'Oct':'10' , 'Nov':'11', 'Dec':'12' }\n",
    "\n",
    "# group data\n",
    "df_group['Begin_Date']=df_group.begin_date.apply(lambda x: Convert_Date(x))\n",
    "df_group['Begin_Tick']=df_group.begin_date.apply(lambda x: Date2Ticks(x))\n",
    "df_group['SubLine']= df_group.sub_line.apply(lambda x: int(x[14:]))\n",
    "df_group['Area']= df_group.area.apply(lambda x: int(x[11:]))\n",
    "df_group['name']= df_group.area.apply(lambda x: len(x))\n",
    "df_group['group_id']=df_group.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_airline['group_id']=df_airline.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_order['group_id']=df_order.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_day_schedule['group_id']=df_day_schedule.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_train['order_id']=df_train.order_id.apply(lambda x: Convert_orderid(x))\n",
    "df_result['order_id']=df_result.order_id.apply(lambda x: Convert_orderid(x))\n",
    "df_airline = df_airline.drop_duplicates(subset='group_id', keep='first', inplace=False)\n",
    "\n",
    "\n",
    "group_used_cols=['group_id','Begin_Date','Begin_Tick','days','Area','SubLine','price', 'product_name']\n",
    "df_group_0 = df_group[group_used_cols].merge(df_airline, on='group_id')\n",
    "df_order_1 = df_order.merge(df_group_0, on='group_id')\n",
    "\n",
    "# for order data\n",
    "df_order_1['Order_Date']=df_order_1.order_date.apply(lambda x: Convert_Date(x))\n",
    "df_order_1['Order_Tick']=df_order_1.order_date.apply(lambda x: Date2Ticks(x))\n",
    "df_order_1['order_id']=df_order_1.order_id.apply(lambda x: Convert_orderid(x))\n",
    "df_order_1['Source_1']= df_order_1.source_1.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Source_2']= df_order_1.source_2.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Unit']= df_order_1.unit.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Begin_Date']=pd.to_datetime(df_order_1['Begin_Date'])\n",
    "df_order_1['Order_Date']=pd.to_datetime(df_order_1['Order_Date'])\n",
    "df_order_1['PreDays']=(df_order_1['Begin_Date']-df_order_1['Order_Date']).dt.days\n",
    "df_order_1['Begin_Date_Weekday']= df_order_1['Begin_Date'].dt.dayofweek\n",
    "df_order_1['Order_Date_Weekday']= df_order_1['Order_Date'].dt.dayofweek\n",
    "df_order_1['Return_Date_Weekday']= (df_order_1['Begin_Date'].dt.dayofweek+df_order_1['days'])%7\n",
    "df_order_1['tick_diff'] = (df_order_1['Begin_Tick'] - df_order_1['Order_Tick'])/10000\n",
    "df_order_1['price'] = df_order_1['price']/1000\n",
    "\n",
    "order_used_columns=['order_id', 'group_id','tick_diff', 'Source_1', 'Source_2', 'Unit',\n",
    "'people_amount', 'Begin_Tick','days', 'Order_Tick', 'Area', 'SubLine', 'price','PreDays','Begin_Date_Weekday', \n",
    "'Order_Date_Weekday', 'Return_Date_Weekday', 'fly_t', 'fly_date',\n",
    "\"src_airport\", \"arrive_t\", \"arrive_date\", \"dst_airport\", 'product_name']\n",
    "\n",
    "# df_order_2=df_order_1[order_used_columns].merge(df_day_schedule[['group_id','title']], on='group_id')\n",
    "df_order_2=df_order_1[order_used_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 297020, 99895 training, testing data\n",
      "Got 296237, 99736 training, testing data\n"
     ]
    }
   ],
   "source": [
    "# train/test data\n",
    "print(\"Got %d, %d training, testing data\" % (len(df_train), len(df_result)))\n",
    "df_train_1=df_train.merge(df_order_2,on='order_id')\n",
    "df_result_1=df_result.merge(df_order_2,on='order_id')\n",
    "print(\"Got %d, %d training, testing data\" % (len(df_train_1), len(df_result_1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296237, 21)\n"
     ]
    }
   ],
   "source": [
    "# train/test data\n",
    "df_train_1=df_train.merge(df_order_2,on='order_id')\n",
    "df_result_1=df_result.merge(df_order_2,on='order_id')\n",
    "\n",
    "Y=df_train_1['deal_or_not'].values.tolist()\n",
    "swX_tmp = (df_train_1['product_name']).values.tolist()\n",
    "Xid = df_train_1['order_id'].values.tolist()\n",
    "del df_train_1['deal_or_not'] \n",
    "del df_train_1['product_name']\n",
    "del df_train_1['group_id'] \n",
    "del df_train_1['order_id']\n",
    "X = df_train_1.values.tolist()\n",
    "\n",
    "rid = df_result_1['order_id'].values.tolist()\n",
    "swrx = (df_result_1['product_name']).values.tolist()\n",
    "del df_result_1['product_name']\n",
    "del df_result_1['deal_or_not']\n",
    "del df_result_1['order_id']\n",
    "del df_result_1['group_id']\n",
    "\n",
    "rx = df_result_1.values.tolist()\n",
    "\n",
    "\n",
    "sX, sY, Xid =np.asarray(X), np.asarray(Y), np.asarray(Xid)\n",
    "rx,rid = np.asarray(rx), np.asarray(rid)\n",
    "X,Y, swX=[],[], []\n",
    "for i in range(len(sY)):\n",
    "   # if (int(Xid[i])<=204000):\n",
    "        X.append(sX[i,:])\n",
    "        Y.append(sY[i])\n",
    "        swX.append(swX_tmp[i])\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "def text_to_index_array(p_new_dic, p_sen):  # 文本转为索引数字模式\n",
    "    new_sentences = []\n",
    "    for sen in p_sen:\n",
    "        new_sen = []\n",
    "        for word in str(sen):\n",
    "            try:\n",
    "                new_sen.append(p_new_dic[word])  # 单词转索引数字\n",
    "            except:\n",
    "                new_sen.append(0)  # 索引字典里没有的词转为数字0\n",
    "        new_sentences.append(new_sen)\n",
    "\n",
    "    return np.array(new_sentences)\n",
    "\n",
    "\n",
    "wX = text_to_index_array(index_dict, swX)\n",
    "wrx = text_to_index_array(index_dict, swrx)\n",
    "wX = sequence.pad_sequences(wX, maxlen=60)\n",
    "wrx = sequence.pad_sequences(wrx, maxlen=60)\n",
    "\n",
    "\n",
    "# X=np.concatenate([X, wX], axis=1)\n",
    "# rx=np.concatenate([rx, wrx], axis=1)\n",
    "# xlen=len(X)\n",
    "# from sklearn.preprocessing import normalize\n",
    "# Xtmp=normalize(np.concatenate([X, rx], axis=0),norm='max', axis=0)\n",
    "# X=Xtmp[:xlen]\n",
    "# rx=Xtmp[xlen:]\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# np.save(\"data.npy\", [X,Y,rx])\n",
    "# [X,Y,rx] = np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(295237, 21)\n",
      "Ridge model done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1217\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:125: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number1.060688e-21\n",
      "  overwrite_a=True).T\n",
      "C:\\Users\\m1217\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso model done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m1217\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet model done\n",
      "GBTree model done\n",
      "XGBTree model done\n",
      "0:\tlearn: 0.4391621\ttotal: 102ms\tremaining: 50.7s\n",
      "200:\tlearn: 0.3779972\ttotal: 9.67s\tremaining: 14.4s\n",
      "400:\tlearn: 0.3765536\ttotal: 19.2s\tremaining: 4.75s\n",
      "499:\tlearn: 0.3760102\ttotal: 24s\tremaining: 0us\n",
      "CatBoosting model done\n",
      "Random Forest model done\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[500]\tvalid_0's auc: 0.624298\n",
      "[1000]\tvalid_0's auc: 0.627953\n",
      "[1500]\tvalid_0's auc: 0.634957\n",
      "[2000]\tvalid_0's auc: 0.639883\n",
      "[2500]\tvalid_0's auc: 0.642465\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2487]\tvalid_0's auc: 0.642477\n",
      "Light GBM model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge, RidgeCV, ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from catboost import Pool, CatBoostRegressor, cv\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "len1 = len(Y)\n",
    "tind = np.zeros(len1, np.int)\n",
    "for i in range(len1):\n",
    "    tind[i]=i\n",
    "import random as rn\n",
    "rn.Random(4).shuffle(tind)\n",
    "\n",
    "train_x, train_y = X[tind[1000:],:], Y[tind[1000:]]\n",
    "valid_x, valid_y = X[tind[:1000],:], Y[tind[:1000]]\n",
    "print(train_x.shape)\n",
    "\n",
    "\n",
    "model_ridge = Ridge(alpha = 0.1)\n",
    "model_ridge.fit(train_x, train_y)\n",
    "print('Ridge model done')\n",
    "\n",
    "model_lasso = Lasso(alpha = 0.005)\n",
    "model_lasso.fit(train_x, train_y)\n",
    "print('Lasso model done')\n",
    "\n",
    "model_en = ElasticNet(alpha = 0.005)\n",
    "model_en.fit(train_x, train_y)\n",
    "print('ElasticNet model done')\n",
    "\n",
    "model_gbr = GradientBoostingRegressor(n_estimators=100, \n",
    "                                      learning_rate=0.03,\n",
    "                                      max_depth=3, \n",
    "                                      max_features='sqrt',\n",
    "                                      min_samples_leaf=50, \n",
    "                                      min_samples_split=10, \n",
    "                                      loss='huber',\n",
    "                                      random_state=5)\n",
    "model_gbr.fit(train_x, train_y)\n",
    "print('GBTree model done')\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.2,\n",
    "                             learning_rate=0.03,\n",
    "                             max_depth=4,verbose=200,\n",
    "                             n_estimators=100)\n",
    "model_xgb.fit(train_x, train_y)\n",
    "print('XGBTree model done')\n",
    "\n",
    "model_cat = CatBoostRegressor(iterations=500,custom_metric='AUC',\n",
    "                              learning_rate=0.05,\n",
    "                              depth=3,\n",
    "                              l2_leaf_reg=20,\n",
    "                              border_count=15,\n",
    "                              loss_function='RMSE',\n",
    "                              verbose=200)\n",
    "model_cat.fit(train_x, train_y)\n",
    "print('CatBoosting model done')\n",
    "\n",
    "model_rf = RandomForestRegressor(max_depth=4, bootstrap = True, n_estimators=50, max_features=\"auto\", n_jobs=4)\n",
    "model_rf.fit(train_x, train_y)\n",
    "print('Random Forest model done')\n",
    "\n",
    "params = {\n",
    "'nthread': 8, 'boosting_type': 'dart','objective': 'regression', 'metric': 'auc', \n",
    "'learning_rate': 0.01, 'num_leaves': 40,\n",
    "'max_depth': 3, 'subsample': 0.5, 'feature_fraction': 0.5, \n",
    "'min_split_gain': 0.09, 'min_child_weight': 9.5,\n",
    "'drop_rate':0.5, 'skip_drop':0.5, 'max_drop':5, 'uniform_drop':False, \n",
    "'xgboost_dart_mode':True, 'drop_seed':5 }\n",
    "\n",
    "dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "dval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain)\n",
    "bst = lgb.train(params, dtrain, num_boost_round=2500, valid_sets=[dval], early_stopping_rounds=1000, \n",
    "                verbose_eval=500)\n",
    "print('Light GBM model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 297020, 99895 training, testing data\n",
      "Got 296237, 99736 training, testing data\n",
      "AUC Ridge : 0.580105\n",
      "AUC Lasso: 0.579050\n",
      "AUC ENet: 0.579585\n",
      "AUC GBR: 0.637086\n",
      "AUC XGBR: 0.648177\n",
      "AUC RandomForest: 0.619821\n",
      "AUC CATBoost: 0.669745\n",
      "AUC LightGBM: 0.673441\n",
      "[0.67344079 0.66974472 0.64817692 0.63708568 0.61982148 0.58010452\n",
      " 0.57958532 0.57904961]\n",
      "AUC KNN: 0.668756\n",
      "(99736,)\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "preds.append(model_ridge.predict(X))\n",
    "preds.append(model_lasso.predict(X))\n",
    "preds.append(model_en.predict(X))\n",
    "preds.append(model_gbr.predict(X))\n",
    "preds.append(model_xgb.predict(X))\n",
    "preds.append(model_rf.predict(X))\n",
    "preds.append(model_cat.predict(X))\n",
    "preds.append(bst.predict(X, num_iteration=bst.best_iteration))\n",
    "\n",
    "# train/test data\n",
    "print(\"Got %d, %d training, testing data\" % (len(df_train), len(df_result)))\n",
    "df_train_1=df_train.merge(df_order_2,on='order_id')\n",
    "df_result_1=df_result.merge(df_order_2,on='order_id')\n",
    "print(\"Got %d, %d training, testing data\" % (len(df_train_1), len(df_result_1)))\n",
    "Y=df_train_1['deal_or_not'].values.tolist()\n",
    "\n",
    "df_train_1['deal_or_not'] =Y\n",
    "\n",
    "for i in range(8):\n",
    "    df_train_1['pred%d'%(i)] = preds[i]\n",
    "\n",
    "pred_scores = np.zeros((8))\n",
    "for i in range(8):\n",
    "    pred_scores[i] =roc_auc_score(Y, preds[i])\n",
    "\n",
    "print('AUC Ridge : %.6f' % (roc_auc_score(Y, preds[0])))\n",
    "print('AUC Lasso: %.6f' % (roc_auc_score(Y, preds[1])))\n",
    "print('AUC ENet: %.6f' % (roc_auc_score(Y, preds[2])))\n",
    "print('AUC GBR: %.6f' % (roc_auc_score(Y, preds[3])))\n",
    "print('AUC XGBR: %.6f' % (roc_auc_score(Y, preds[4])))\n",
    "print('AUC RandomForest: %.6f' % (roc_auc_score(Y, preds[5])))\n",
    "print('AUC CATBoost: %.6f' % (roc_auc_score(Y, preds[6])))\n",
    "print('AUC LightGBM: %.6f' % (roc_auc_score(Y, preds[7])))\n",
    "\n",
    "idx1 = np.array(pred_scores).argsort()[::-1]\n",
    "final_val=preds[idx1[0]]\n",
    "print(pred_scores[idx1])\n",
    "for i in range(1,3):\n",
    "    final_val += preds[idx1[i]]\n",
    "final_val /= 3.0\n",
    "print('AUC KNN: %.6f' % (roc_auc_score(Y, final_val)))\n",
    "\n",
    "\n",
    "sub_preds=[]\n",
    "sub_preds.append(model_ridge.predict(rx))\n",
    "sub_preds.append(model_lasso.predict(rx))\n",
    "sub_preds.append(model_en.predict(rx))\n",
    "sub_preds.append(model_gbr.predict(rx))\n",
    "sub_preds.append(model_xgb.predict(rx))\n",
    "sub_preds.append(model_rf.predict(rx))\n",
    "sub_preds.append(model_cat.predict(rx))\n",
    "sub_preds.append(bst.predict(rx, num_iteration=bst.best_iteration))\n",
    "\n",
    "for i in range(8):\n",
    "    df_result_1['pred%d'%(i)] = sub_preds[i]\n",
    "    \n",
    "df_train_1.to_csv('myTrain3.csv', index=False)\n",
    "df_result_1.to_csv('myTest3.csv', index=False)\n",
    "\n",
    "idx1 = np.array(pred_scores).argsort()\n",
    "final_res=sub_preds[idx1[0]]\n",
    "print(final_res.shape)\n",
    "for i in range(1,3):\n",
    "    final_res += sub_preds[idx1[i]]\n",
    "final_res /= 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1000, 296237]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-49b933b879b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mpredR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"order_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"outer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mpredR\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'deal_or_not'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeal_or_not\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnanMapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpredR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"output/lgb_dart_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    354\u001b[0m     return _average_binary_score(\n\u001b[0;32m    355\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[1;32m--> 328\u001b[1;33m                                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \"\"\"\n\u001b[0;32m    617\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[1;32m--> 618\u001b[1;33m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[1;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0} format is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 235\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1000, 296237]"
     ]
    }
   ],
   "source": [
    "def nanMapping(x):\n",
    "    if np.isnan(x):\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "app_test = pd.read_csv('testing-set.csv', usecols=['order_id'])\n",
    "predF=pd.DataFrame({\"order_id\":rid, \"deal_or_not\":final_res})\n",
    "predR = pd.DataFrame({\"order_id\":app_test['order_id']})\n",
    "predR=predR.merge(predF, on=\"order_id\",how=\"outer\")\n",
    "predR['deal_or_not'] = predR.deal_or_not.apply(lambda x: nanMapping(x))\n",
    "predR.to_csv(\"output/lgb_dart_\" + str(roc_auc_score(valid_y, final_val)) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
