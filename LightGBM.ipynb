{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pickle\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import time, datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import gc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import jieba, pdb\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "\n",
    "jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "# load stopwords set\n",
    "stopword_set = set()\n",
    "with open('jieba_dict/stopwords.txt','r', encoding='utf-8') as stopwords:\n",
    "    for stopword in stopwords:\n",
    "        stopword_set.add(stopword.strip('\\n'))\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"word2vec2.model\")\n",
    "\n",
    "\n",
    "def create_dictionaries(p_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(p_model.wv.vocab.keys(), allow_update=True)\n",
    "    w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2indx.keys()}  # 词语的词向量\n",
    "    return w2indx, w2vec\n",
    "\n",
    "def Convert_uniqueid(x, sets):\n",
    "    return np.where(sets==x)[0][0]\n",
    "\n",
    "def parsePathalias(x):\n",
    "    if x == 'None':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x)\n",
    "\n",
    "def Convert_Date(x):\n",
    "    timestamp = x\n",
    "    timeArray = time.localtime(timestamp)\n",
    "    datetime = time.strftime(\"%Y-%m-%d %H:%M:%S\", timeArray)\n",
    "    return datetime\n",
    "\n",
    "def Date2Ticks(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = str(Year+'/'+Month+'/'+Day)\n",
    "    return time.mktime(datetime.datetime.strptime(date1, \"%Y/%m/%d\").timetuple())\n",
    "\n",
    "index_dict, word_vectors= create_dictionaries(model)\n",
    "output = open(\"wordwmbedding.pkl\", 'wb')\n",
    "pickle.dump(index_dict, output)  # 索引字典\n",
    "pickle.dump(word_vectors, output)  # 词向量字典\n",
    "output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df_tag = pd.read_csv(\"train/train_tags2.txt\", delimiter=\",\",encoding = \"utf-8\")\n",
    "df_add = pd.read_csv(\"train/train_additional.txt\",delimiter=\" \")\n",
    "df_cat = pd.read_csv(\"train/train_category.txt\",delimiter=\" \")\n",
    "df_ts = pd.read_csv(\"train/train_temporalspatial.txt\",delimiter=\" \")\n",
    "df_label = pd.read_csv(\"train/train_label.txt\",delimiter=\" \")\n",
    "\n",
    "'''\n",
    "Cat: Uid Pid Category Subcategory Concept\n",
    "TAG: Uid Pid Title Mediatype Alltags\n",
    "TS:  Uid Pid Postdate Latitude Longitude Geoaccuracy\n",
    "Add: Uid Pid Pathalias Ispublic Mediastatus\n",
    "\n",
    "'''\n",
    "# date Conversion\n",
    "uidset =      np.unique(np.asarray(df_cat['Uid'].values.tolist()))\n",
    "pidset =      np.unique(np.asarray(df_cat['Pid'].values.tolist()))\n",
    "Category =    np.unique(np.asarray(df_cat['Category'].values.tolist()))\n",
    "Subcategory = np.unique(np.asarray(df_cat['Subcategory'].values.tolist()))\n",
    "Concept =     np.unique(np.asarray(df_cat['Concept'].values.tolist()))\n",
    "\n",
    "\n",
    "month = {'Jan': '01', 'Feb': '02' , 'Mar':'03' ,'Apr': '04', \n",
    "'May': '05', 'Jun': '06' , 'Jul': '07' , 'Aug':'08', \n",
    "'Sep':'09', 'Oct':'10' , 'Nov':'11', 'Dec':'12' }\n",
    "\n",
    "# group data\n",
    "df_cat['Uid']=df_cat.Uid.apply(lambda x: Convert_uniqueid(x, uidset))\n",
    "df_cat['Pid']=df_cat.Pid.apply(lambda x: Convert_uniqueid(x, pidset))\n",
    "df_cat['Category']= df_cat.Category.apply(lambda x: Convert_uniqueid(x, Category))\n",
    "df_cat['Subcategory']= df_cat.Subcategory.apply(lambda x:  Convert_uniqueid(x, Subcategory))\n",
    "df_cat['Concept']= df_cat.Concept.apply(lambda x: Convert_uniqueid(x, Concept))\n",
    "\n",
    "df_add['Uid']=df_add.Uid.apply(lambda x: Convert_uniqueid(x, uidset))\n",
    "df_add['Pid']=df_add.Pid.apply(lambda x: Convert_uniqueid(x, pidset))\n",
    "df_add['Pathalias']=df_add.Pathalias.apply(lambda x: parsePathalias(x))\n",
    "df_add['Ispublic']=df_add.Ispublic.apply(lambda x: int(x))\n",
    "df_add['Mediastatus']=df_add.Mediastatus.apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "df_ts['Uid']=df_ts.Uid.apply(lambda x: Convert_uniqueid(x, uidset))\n",
    "df_ts['Pid']=df_ts.Pid.apply(lambda x: Convert_uniqueid(x, pidset))\n",
    "df_ts['Postdate']=df_ts.Pid.apply(lambda x: int(x))\n",
    "df_ts['Latitude']=df_ts.Pid.apply(lambda x: float(x))\n",
    "df_ts['Longitude']=df_ts.Pid.apply(lambda x: float(x))\n",
    "df_ts['Geoaccuracy']=df_ts.Pid.apply(lambda x: float(x))\n",
    "\n",
    "df_tag['Uid']=df_tag.Uid.apply(lambda x: Convert_uniqueid(x, uidset))\n",
    "df_tag['Pid']=df_tag.Pid.apply(lambda x: Convert_uniqueid(x, pidset))\n",
    "df_tag['Title']= df_tag.Title.apply(lambda x: len(x))\n",
    "\n",
    "df_label['score'] = df_label.score.apply(lambda x: float(x))\n",
    "\n",
    "# df_airline = df_airline.drop_duplicates(subset='group_id', keep='first', inplace=False)\n",
    "\n",
    "# group_used_cols=['group_id','Begin_Date','Begin_Tick','days','Area','SubLine','price', 'product_name']\n",
    "# df_group_0 = df_group[group_used_cols].merge(df_airline, on='group_id')\n",
    "\n",
    "# # for order data\n",
    "# df_order_1['Order_Date']=df_order_1.order_date.apply(lambda x: Convert_Date(x))\n",
    "# df_order_1['Order_Tick']=df_order_1.order_date.apply(lambda x: Date2Ticks(x))\n",
    "# df_order_1['order_id']=df_order_1.order_id.apply(lambda x: Convert_orderid(x))\n",
    "# df_order_1['Source_1']= df_order_1.source_1.apply(lambda x: int(x[11:]))\n",
    "# df_order_1['Source_2']= df_order_1.source_2.apply(lambda x: int(x[11:]))\n",
    "# df_order_1['Unit']= df_order_1.unit.apply(lambda x: int(x[11:]))\n",
    "# df_order_1['Begin_Date']=pd.to_datetime(df_order_1['Begin_Date'])\n",
    "# df_order_1['Order_Date']=pd.to_datetime(df_order_1['Order_Date'])\n",
    "# df_order_1['PreDays']=(df_order_1['Begin_Date']-df_order_1['Order_Date']).dt.days\n",
    "# df_order_1['Begin_Date_Weekday']= df_order_1['Begin_Date'].dt.dayofweek\n",
    "# df_order_1['Order_Date_Weekday']= df_order_1['Order_Date'].dt.dayofweek\n",
    "# df_order_1['Return_Date_Weekday']= (df_order_1['Begin_Date'].dt.dayofweek+df_order_1['days'])%7\n",
    "# df_order_1['tick_diff'] = (df_order_1['Begin_Tick'] - df_order_1['Order_Tick'])/10000\n",
    "# df_order_1['price'] = df_order_1['price']/1000\n",
    "\n",
    "# order_used_columns=['order_id', 'group_id','tick_diff', 'Source_1', 'Source_2', 'Unit',\n",
    "# 'people_amount', 'Begin_Tick','days', 'Order_Tick', 'Area', 'SubLine', 'price','PreDays','Begin_Date_Weekday', \n",
    "# 'Order_Date_Weekday', 'Return_Date_Weekday', 'fly_t', 'fly_date',\n",
    "# \"src_airport\", \"arrive_t\", \"arrive_date\", \"dst_airport\", 'product_name']\n",
    "\n",
    "# df_order_2=df_order_1[order_used_columns].merge(df_day_schedule[['group_id','title']], on='group_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 305613 training with 305613 labels\n",
      "Got 305613 training with 305613 labels\n"
     ]
    }
   ],
   "source": [
    "print(\"Got %d training with %d labels\" % (len(df_cat), len(df_label)))\n",
    "\n",
    "\n",
    "df_all = df_cat.merge(df_tag, on=['Uid', 'Pid'])\n",
    "df_all2 = df_ts.merge(df_all, on=['Uid', 'Pid'])\n",
    "df_all = df_tag.merge(df_all2, on=['Uid', 'Pid'])\n",
    "\n",
    "# train/test data\n",
    "print(\"Got %d training with %d labels\" % (len(df_all), len(df_label)))\n",
    "\n",
    "# del df_all['Pid'] \n",
    "# del df_all['Uid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test data\n",
    "# df_train_1=df_train.merge(df_order_2,on='order_id')\n",
    "# df_result_1=df_result.merge(df_order_2,on='order_id')\n",
    "# df_train_1.to_csv('myTrain2.csv', index=False)\n",
    "# df_result_1.to_csv('myTest2.csv', index=False)\n",
    "\n",
    "# Y=df_train_1['deal_or_not'].values.tolist()\n",
    "# swX_tmp = (df_train_1['product_name']).values.tolist()\n",
    "# Xid = df_train_1['order_id'].values.tolist()\n",
    "# del df_train_1['deal_or_not'] \n",
    "# del df_train_1['product_name']\n",
    "# del df_train_1['group_id'] \n",
    "# del df_train_1['order_id']\n",
    "# X = df_train_1.values.tolist()\n",
    "\n",
    "# rid = df_result_1['order_id'].values.tolist()\n",
    "# swrx = (df_result_1['product_name']).values.tolist()\n",
    "# del df_result_1['product_name']\n",
    "# del df_result_1['deal_or_not']\n",
    "# del df_result_1['order_id']\n",
    "# del df_result_1['group_id']\n",
    "\n",
    "# rx = df_result_1.values.tolist()\n",
    "\n",
    "\n",
    "# sX, sY, Xid =np.asarray(X), np.asarray(Y), np.asarray(Xid)\n",
    "# rx,rid = np.asarray(rx), np.asarray(rid)\n",
    "# X,Y, swX=[],[], []\n",
    "# for i in range(len(sY)):\n",
    "#    # if (int(Xid[i])<=204000):\n",
    "#         X.append(sX[i,:])\n",
    "#         Y.append(sY[i])\n",
    "#         swX.append(swX_tmp[i])\n",
    "# X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "# def text_to_index_array(p_new_dic, p_sen):  # 文本转为索引数字模式\n",
    "#     new_sentences = []\n",
    "#     for sen in p_sen:\n",
    "#         new_sen = []\n",
    "#         for word in str(sen):\n",
    "#             try:\n",
    "#                 new_sen.append(p_new_dic[word])  # 单词转索引数字\n",
    "#             except:\n",
    "#                 new_sen.append(0)  # 索引字典里没有的词转为数字0\n",
    "#         new_sentences.append(new_sen)\n",
    "\n",
    "#     return np.array(new_sentences)\n",
    "\n",
    "\n",
    "# wX = text_to_index_array(index_dict, swX)\n",
    "# wrx = text_to_index_array(index_dict, swrx)\n",
    "# wX = sequence.pad_sequences(wX, maxlen=60)\n",
    "# wrx = sequence.pad_sequences(wrx, maxlen=60)\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "# pt_X = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "# sc_y = StandardScaler()\n",
    "# sc_X = StandardScaler()\n",
    "# X = sc_X.fit_transform(X)\n",
    "# rx = sc_X.transform(rx)\n",
    "\n",
    "# # X=np.concatenate([X, wX], axis=1)\n",
    "# # rx=np.concatenate([rx, wrx], axis=1)\n",
    "# # xlen=len(X)\n",
    "# # from sklearn.preprocessing import normalize\n",
    "# # Xtmp=normalize(np.concatenate([X, rx], axis=0),norm='max', axis=0)\n",
    "# # X=Xtmp[:xlen]\n",
    "# # rx=Xtmp[xlen:]\n",
    "\n",
    "# print(X.shape)\n",
    "\n",
    "# # np.save(\"data.npy\", [X,Y,rx])\n",
    "# # [X,Y,rx] = np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305613,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Index: [     0      2      3 ... 305610 305611 305612] ,Val Index: [     1     17     48 ... 305582 305596 305605]\n",
      "Training until validation scores don't improve for 2500 rounds.\n",
      "[500]\tvalid_0's l1: 1.47614\tvalid_0's l2: 3.67833\n",
      "[1000]\tvalid_0's l1: 1.36509\tvalid_0's l2: 3.26465\n",
      "[1500]\tvalid_0's l1: 1.30175\tvalid_0's l2: 3.04285\n",
      "[2000]\tvalid_0's l1: 1.25349\tvalid_0's l2: 2.88294\n",
      "[2500]\tvalid_0's l1: 1.21357\tvalid_0's l2: 2.7559\n",
      "[3000]\tvalid_0's l1: 1.17963\tvalid_0's l2: 2.65168\n",
      "[3500]\tvalid_0's l1: 1.15112\tvalid_0's l2: 2.5642\n",
      "[4000]\tvalid_0's l1: 1.126\tvalid_0's l2: 2.49115\n",
      "[4500]\tvalid_0's l1: 1.10501\tvalid_0's l2: 2.42967\n",
      "[5000]\tvalid_0's l1: 1.08777\tvalid_0's l2: 2.37902\n",
      "[5500]\tvalid_0's l1: 1.07094\tvalid_0's l2: 2.32978\n",
      "[6000]\tvalid_0's l1: 1.05681\tvalid_0's l2: 2.29054\n",
      "[6500]\tvalid_0's l1: 1.04187\tvalid_0's l2: 2.24929\n",
      "[7000]\tvalid_0's l1: 1.02833\tvalid_0's l2: 2.21337\n",
      "[7500]\tvalid_0's l1: 1.01709\tvalid_0's l2: 2.18386\n",
      "[8000]\tvalid_0's l1: 1.00674\tvalid_0's l2: 2.15649\n",
      "[8500]\tvalid_0's l1: 0.996782\tvalid_0's l2: 2.12988\n",
      "[9000]\tvalid_0's l1: 0.987353\tvalid_0's l2: 2.10474\n",
      "[9500]\tvalid_0's l1: 0.978739\tvalid_0's l2: 2.08239\n",
      "[10000]\tvalid_0's l1: 0.970047\tvalid_0's l2: 2.05912\n",
      "[10500]\tvalid_0's l1: 0.963193\tvalid_0's l2: 2.04147\n",
      "[11000]\tvalid_0's l1: 0.95633\tvalid_0's l2: 2.0237\n",
      "[11500]\tvalid_0's l1: 0.948636\tvalid_0's l2: 2.00394\n",
      "[12000]\tvalid_0's l1: 0.94307\tvalid_0's l2: 1.98962\n",
      "[12500]\tvalid_0's l1: 0.937914\tvalid_0's l2: 1.97678\n",
      "[13000]\tvalid_0's l1: 0.932052\tvalid_0's l2: 1.96203\n",
      "[13500]\tvalid_0's l1: 0.926101\tvalid_0's l2: 1.94739\n",
      "[14000]\tvalid_0's l1: 0.920487\tvalid_0's l2: 1.93308\n",
      "[14500]\tvalid_0's l1: 0.915372\tvalid_0's l2: 1.9201\n",
      "[15000]\tvalid_0's l1: 0.911263\tvalid_0's l2: 1.9099\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\tvalid_0's l1: 0.911263\tvalid_0's l2: 1.9099\n",
      "Fold  1 rank corr : 0.835361\n",
      "Train Index: [     0      1      2 ... 305610 305611 305612] ,Val Index: [    21     26     44 ... 305579 305585 305607]\n",
      "Training until validation scores don't improve for 2500 rounds.\n",
      "[500]\tvalid_0's l1: 1.48119\tvalid_0's l2: 3.65541\n",
      "[1000]\tvalid_0's l1: 1.37082\tvalid_0's l2: 3.2439\n",
      "[1500]\tvalid_0's l1: 1.30017\tvalid_0's l2: 2.99471\n",
      "[2000]\tvalid_0's l1: 1.24951\tvalid_0's l2: 2.82365\n",
      "[2500]\tvalid_0's l1: 1.21086\tvalid_0's l2: 2.69946\n",
      "[3000]\tvalid_0's l1: 1.17536\tvalid_0's l2: 2.58992\n",
      "[3500]\tvalid_0's l1: 1.14651\tvalid_0's l2: 2.50321\n",
      "[4000]\tvalid_0's l1: 1.12166\tvalid_0's l2: 2.43042\n",
      "[4500]\tvalid_0's l1: 1.10101\tvalid_0's l2: 2.37015\n",
      "[5000]\tvalid_0's l1: 1.08348\tvalid_0's l2: 2.31997\n",
      "[5500]\tvalid_0's l1: 1.06677\tvalid_0's l2: 2.27402\n",
      "[6000]\tvalid_0's l1: 1.05188\tvalid_0's l2: 2.23261\n",
      "[6500]\tvalid_0's l1: 1.03716\tvalid_0's l2: 2.19128\n",
      "[7000]\tvalid_0's l1: 1.02412\tvalid_0's l2: 2.15479\n",
      "[7500]\tvalid_0's l1: 1.01238\tvalid_0's l2: 2.12241\n",
      "[8000]\tvalid_0's l1: 1.00079\tvalid_0's l2: 2.08999\n",
      "[8500]\tvalid_0's l1: 0.99057\tvalid_0's l2: 2.06195\n",
      "[9000]\tvalid_0's l1: 0.981145\tvalid_0's l2: 2.0367\n",
      "[9500]\tvalid_0's l1: 0.972693\tvalid_0's l2: 2.0146\n",
      "[10000]\tvalid_0's l1: 0.964157\tvalid_0's l2: 1.99263\n",
      "[10500]\tvalid_0's l1: 0.956436\tvalid_0's l2: 1.97374\n",
      "[11000]\tvalid_0's l1: 0.948864\tvalid_0's l2: 1.95531\n",
      "[11500]\tvalid_0's l1: 0.942103\tvalid_0's l2: 1.93909\n",
      "[12000]\tvalid_0's l1: 0.93663\tvalid_0's l2: 1.92601\n",
      "[12500]\tvalid_0's l1: 0.930681\tvalid_0's l2: 1.9119\n",
      "[13000]\tvalid_0's l1: 0.925007\tvalid_0's l2: 1.89846\n",
      "[13500]\tvalid_0's l1: 0.91975\tvalid_0's l2: 1.88592\n",
      "[14000]\tvalid_0's l1: 0.914418\tvalid_0's l2: 1.87326\n",
      "[14500]\tvalid_0's l1: 0.908347\tvalid_0's l2: 1.85861\n",
      "[15000]\tvalid_0's l1: 0.902907\tvalid_0's l2: 1.84499\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\tvalid_0's l1: 0.902907\tvalid_0's l2: 1.84499\n",
      "Fold  2 rank corr : 0.840677\n",
      "Train Index: [     0      1      2 ... 305610 305611 305612] ,Val Index: [     6     16     27 ... 305558 305576 305577]\n",
      "Training until validation scores don't improve for 2500 rounds.\n",
      "[500]\tvalid_0's l1: 1.48138\tvalid_0's l2: 3.66432\n",
      "[1000]\tvalid_0's l1: 1.36869\tvalid_0's l2: 3.24415\n",
      "[1500]\tvalid_0's l1: 1.30109\tvalid_0's l2: 3.00449\n",
      "[2000]\tvalid_0's l1: 1.25097\tvalid_0's l2: 2.83732\n",
      "[2500]\tvalid_0's l1: 1.21034\tvalid_0's l2: 2.70922\n",
      "[3000]\tvalid_0's l1: 1.17501\tvalid_0's l2: 2.60196\n",
      "[3500]\tvalid_0's l1: 1.14731\tvalid_0's l2: 2.51872\n",
      "[4000]\tvalid_0's l1: 1.1227\tvalid_0's l2: 2.44573\n",
      "[4500]\tvalid_0's l1: 1.1015\tvalid_0's l2: 2.38433\n",
      "[5000]\tvalid_0's l1: 1.08426\tvalid_0's l2: 2.33596\n",
      "[5500]\tvalid_0's l1: 1.06897\tvalid_0's l2: 2.29287\n",
      "[6000]\tvalid_0's l1: 1.05461\tvalid_0's l2: 2.25169\n",
      "[6500]\tvalid_0's l1: 1.04003\tvalid_0's l2: 2.21068\n",
      "[7000]\tvalid_0's l1: 1.02634\tvalid_0's l2: 2.1725\n",
      "[7500]\tvalid_0's l1: 1.01499\tvalid_0's l2: 2.14147\n",
      "[8000]\tvalid_0's l1: 1.00267\tvalid_0's l2: 2.10884\n",
      "[8500]\tvalid_0's l1: 0.992383\tvalid_0's l2: 2.08261\n",
      "[9000]\tvalid_0's l1: 0.982932\tvalid_0's l2: 2.05732\n",
      "[9500]\tvalid_0's l1: 0.973625\tvalid_0's l2: 2.03317\n",
      "[10000]\tvalid_0's l1: 0.965388\tvalid_0's l2: 2.01192\n",
      "[10500]\tvalid_0's l1: 0.957902\tvalid_0's l2: 1.99247\n",
      "[11000]\tvalid_0's l1: 0.950268\tvalid_0's l2: 1.97258\n",
      "[11500]\tvalid_0's l1: 0.94374\tvalid_0's l2: 1.95601\n",
      "[12000]\tvalid_0's l1: 0.93789\tvalid_0's l2: 1.94142\n",
      "[12500]\tvalid_0's l1: 0.932819\tvalid_0's l2: 1.92955\n",
      "[13000]\tvalid_0's l1: 0.927591\tvalid_0's l2: 1.91629\n",
      "[13500]\tvalid_0's l1: 0.922833\tvalid_0's l2: 1.90427\n",
      "[14000]\tvalid_0's l1: 0.91805\tvalid_0's l2: 1.89248\n",
      "[14500]\tvalid_0's l1: 0.912776\tvalid_0's l2: 1.87977\n",
      "[15000]\tvalid_0's l1: 0.907418\tvalid_0's l2: 1.86696\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\tvalid_0's l1: 0.907418\tvalid_0's l2: 1.86696\n",
      "Fold  3 rank corr : 0.836484\n",
      "Train Index: [     0      1      2 ... 305610 305611 305612] ,Val Index: [     5      7      8 ... 305569 305571 305578]\n",
      "Training until validation scores don't improve for 2500 rounds.\n",
      "[500]\tvalid_0's l1: 1.48308\tvalid_0's l2: 3.65031\n",
      "[1000]\tvalid_0's l1: 1.37029\tvalid_0's l2: 3.22693\n",
      "[1500]\tvalid_0's l1: 1.30468\tvalid_0's l2: 2.99828\n",
      "[2000]\tvalid_0's l1: 1.25514\tvalid_0's l2: 2.8387\n",
      "[2500]\tvalid_0's l1: 1.21635\tvalid_0's l2: 2.7164\n",
      "[3000]\tvalid_0's l1: 1.18354\tvalid_0's l2: 2.61629\n",
      "[3500]\tvalid_0's l1: 1.15335\tvalid_0's l2: 2.52356\n",
      "[4000]\tvalid_0's l1: 1.12873\tvalid_0's l2: 2.45016\n",
      "[4500]\tvalid_0's l1: 1.1093\tvalid_0's l2: 2.39281\n",
      "[5000]\tvalid_0's l1: 1.09048\tvalid_0's l2: 2.33717\n",
      "[5500]\tvalid_0's l1: 1.0751\tvalid_0's l2: 2.29303\n",
      "[6000]\tvalid_0's l1: 1.0621\tvalid_0's l2: 2.25626\n",
      "[6500]\tvalid_0's l1: 1.0487\tvalid_0's l2: 2.22009\n",
      "[7000]\tvalid_0's l1: 1.03552\tvalid_0's l2: 2.18433\n",
      "[7500]\tvalid_0's l1: 1.02411\tvalid_0's l2: 2.15363\n",
      "[8000]\tvalid_0's l1: 1.01215\tvalid_0's l2: 2.12196\n",
      "[8500]\tvalid_0's l1: 1.00092\tvalid_0's l2: 2.09157\n",
      "[9000]\tvalid_0's l1: 0.990893\tvalid_0's l2: 2.06401\n",
      "[9500]\tvalid_0's l1: 0.980769\tvalid_0's l2: 2.03761\n",
      "[10000]\tvalid_0's l1: 0.97102\tvalid_0's l2: 2.01196\n",
      "[10500]\tvalid_0's l1: 0.963754\tvalid_0's l2: 1.99285\n",
      "[11000]\tvalid_0's l1: 0.956395\tvalid_0's l2: 1.97437\n",
      "[11500]\tvalid_0's l1: 0.949561\tvalid_0's l2: 1.95741\n",
      "[12000]\tvalid_0's l1: 0.943352\tvalid_0's l2: 1.94189\n",
      "[12500]\tvalid_0's l1: 0.937049\tvalid_0's l2: 1.92605\n",
      "[13000]\tvalid_0's l1: 0.931624\tvalid_0's l2: 1.91303\n",
      "[13500]\tvalid_0's l1: 0.926255\tvalid_0's l2: 1.9003\n",
      "[14000]\tvalid_0's l1: 0.920967\tvalid_0's l2: 1.8876\n",
      "[14500]\tvalid_0's l1: 0.91599\tvalid_0's l2: 1.87592\n",
      "[15000]\tvalid_0's l1: 0.91145\tvalid_0's l2: 1.86522\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\tvalid_0's l1: 0.91145\tvalid_0's l2: 1.86522\n",
      "Fold  4 rank corr : 0.835285\n",
      "Train Index: [     0      1      2 ... 305607 305608 305611] ,Val Index: [     4     15     18 ... 305609 305610 305612]\n",
      "Training until validation scores don't improve for 2500 rounds.\n",
      "[500]\tvalid_0's l1: 1.48601\tvalid_0's l2: 3.6812\n",
      "[1000]\tvalid_0's l1: 1.37307\tvalid_0's l2: 3.25824\n",
      "[1500]\tvalid_0's l1: 1.30351\tvalid_0's l2: 3.01855\n",
      "[2000]\tvalid_0's l1: 1.25298\tvalid_0's l2: 2.85428\n",
      "[2500]\tvalid_0's l1: 1.21495\tvalid_0's l2: 2.73161\n",
      "[3000]\tvalid_0's l1: 1.18197\tvalid_0's l2: 2.63003\n",
      "[3500]\tvalid_0's l1: 1.15359\tvalid_0's l2: 2.54482\n",
      "[4000]\tvalid_0's l1: 1.12872\tvalid_0's l2: 2.47278\n",
      "[4500]\tvalid_0's l1: 1.10849\tvalid_0's l2: 2.41534\n",
      "[5000]\tvalid_0's l1: 1.09085\tvalid_0's l2: 2.36538\n",
      "[5500]\tvalid_0's l1: 1.07367\tvalid_0's l2: 2.31581\n",
      "[6000]\tvalid_0's l1: 1.05941\tvalid_0's l2: 2.27472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6500]\tvalid_0's l1: 1.04606\tvalid_0's l2: 2.23756\n",
      "[7000]\tvalid_0's l1: 1.03305\tvalid_0's l2: 2.20194\n",
      "[7500]\tvalid_0's l1: 1.02136\tvalid_0's l2: 2.16982\n",
      "[8000]\tvalid_0's l1: 1.0103\tvalid_0's l2: 2.13976\n",
      "[8500]\tvalid_0's l1: 0.999756\tvalid_0's l2: 2.11201\n",
      "[9000]\tvalid_0's l1: 0.989963\tvalid_0's l2: 2.08658\n",
      "[9500]\tvalid_0's l1: 0.980943\tvalid_0's l2: 2.06354\n",
      "[10000]\tvalid_0's l1: 0.971904\tvalid_0's l2: 2.03998\n",
      "[10500]\tvalid_0's l1: 0.96284\tvalid_0's l2: 2.01564\n",
      "[11000]\tvalid_0's l1: 0.955855\tvalid_0's l2: 1.99764\n",
      "[11500]\tvalid_0's l1: 0.949387\tvalid_0's l2: 1.98168\n",
      "[12000]\tvalid_0's l1: 0.943532\tvalid_0's l2: 1.96773\n",
      "[12500]\tvalid_0's l1: 0.937805\tvalid_0's l2: 1.95421\n",
      "[13000]\tvalid_0's l1: 0.931209\tvalid_0's l2: 1.93822\n",
      "[13500]\tvalid_0's l1: 0.925022\tvalid_0's l2: 1.923\n",
      "[14000]\tvalid_0's l1: 0.919974\tvalid_0's l2: 1.91069\n",
      "[14500]\tvalid_0's l1: 0.9153\tvalid_0's l2: 1.89985\n",
      "[15000]\tvalid_0's l1: 0.910934\tvalid_0's l2: 1.88939\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\tvalid_0's l1: 0.910934\tvalid_0's l2: 1.88939\n",
      "Fold  5 rank corr : 0.836814\n",
      "Train Index: [     0      1      2 ... 305610 305611 305612] ,Val Index: [    14     30     36 ... 305597 305602 305603]\n",
      "Training until validation scores don't improve for 2500 rounds.\n",
      "[500]\tvalid_0's l1: 1.48459\tvalid_0's l2: 3.67139\n",
      "[1000]\tvalid_0's l1: 1.37341\tvalid_0's l2: 3.25096\n",
      "[1500]\tvalid_0's l1: 1.30827\tvalid_0's l2: 3.01854\n",
      "[2000]\tvalid_0's l1: 1.25884\tvalid_0's l2: 2.85401\n",
      "[2500]\tvalid_0's l1: 1.21785\tvalid_0's l2: 2.72145\n",
      "[3000]\tvalid_0's l1: 1.18161\tvalid_0's l2: 2.6103\n",
      "[3500]\tvalid_0's l1: 1.15378\tvalid_0's l2: 2.52529\n",
      "[4000]\tvalid_0's l1: 1.12906\tvalid_0's l2: 2.45131\n",
      "[4500]\tvalid_0's l1: 1.10826\tvalid_0's l2: 2.39209\n",
      "[5000]\tvalid_0's l1: 1.08976\tvalid_0's l2: 2.33943\n",
      "[5500]\tvalid_0's l1: 1.07372\tvalid_0's l2: 2.29417\n",
      "[6000]\tvalid_0's l1: 1.05981\tvalid_0's l2: 2.25529\n",
      "[6500]\tvalid_0's l1: 1.04469\tvalid_0's l2: 2.21359\n",
      "[7000]\tvalid_0's l1: 1.03068\tvalid_0's l2: 2.17504\n",
      "[7500]\tvalid_0's l1: 1.01919\tvalid_0's l2: 2.14358\n",
      "[8000]\tvalid_0's l1: 1.00808\tvalid_0's l2: 2.11364\n",
      "[8500]\tvalid_0's l1: 0.997611\tvalid_0's l2: 2.08657\n",
      "[9000]\tvalid_0's l1: 0.987543\tvalid_0's l2: 2.06009\n",
      "[9500]\tvalid_0's l1: 0.978621\tvalid_0's l2: 2.03746\n",
      "[10000]\tvalid_0's l1: 0.970505\tvalid_0's l2: 2.01552\n",
      "[10500]\tvalid_0's l1: 0.962895\tvalid_0's l2: 1.99601\n",
      "[11000]\tvalid_0's l1: 0.956125\tvalid_0's l2: 1.97879\n",
      "[11500]\tvalid_0's l1: 0.949309\tvalid_0's l2: 1.962\n",
      "[12000]\tvalid_0's l1: 0.943076\tvalid_0's l2: 1.94646\n",
      "[12500]\tvalid_0's l1: 0.936325\tvalid_0's l2: 1.93026\n",
      "[13000]\tvalid_0's l1: 0.930349\tvalid_0's l2: 1.91556\n",
      "[13500]\tvalid_0's l1: 0.924407\tvalid_0's l2: 1.90155\n",
      "[14000]\tvalid_0's l1: 0.919407\tvalid_0's l2: 1.88964\n",
      "[14500]\tvalid_0's l1: 0.913995\tvalid_0's l2: 1.87646\n",
      "[15000]\tvalid_0's l1: 0.909193\tvalid_0's l2: 1.86461\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\tvalid_0's l1: 0.909193\tvalid_0's l2: 1.86461\n",
      "Fold  6 rank corr : 0.837098\n",
      "Train Index: [     1      2      3 ... 305610 305611 305612] ,Val Index: [     0     11     19 ... 305584 305594 305604]\n",
      "Training until validation scores don't improve for 2500 rounds.\n",
      "[500]\tvalid_0's l1: 1.47949\tvalid_0's l2: 3.64177\n",
      "[1000]\tvalid_0's l1: 1.37\tvalid_0's l2: 3.23433\n",
      "[1500]\tvalid_0's l1: 1.30408\tvalid_0's l2: 3.00493\n",
      "[2000]\tvalid_0's l1: 1.25435\tvalid_0's l2: 2.83774\n",
      "[2500]\tvalid_0's l1: 1.21031\tvalid_0's l2: 2.69665\n",
      "[3000]\tvalid_0's l1: 1.17438\tvalid_0's l2: 2.58681\n",
      "[3500]\tvalid_0's l1: 1.14674\tvalid_0's l2: 2.5037\n",
      "[4000]\tvalid_0's l1: 1.12286\tvalid_0's l2: 2.43308\n",
      "[4500]\tvalid_0's l1: 1.10164\tvalid_0's l2: 2.37061\n",
      "[5000]\tvalid_0's l1: 1.08361\tvalid_0's l2: 2.31912\n",
      "[5500]\tvalid_0's l1: 1.06797\tvalid_0's l2: 2.27493\n",
      "[6000]\tvalid_0's l1: 1.05362\tvalid_0's l2: 2.23432\n",
      "[6500]\tvalid_0's l1: 1.03903\tvalid_0's l2: 2.19299\n",
      "[7000]\tvalid_0's l1: 1.02449\tvalid_0's l2: 2.15311\n",
      "[7500]\tvalid_0's l1: 1.01274\tvalid_0's l2: 2.12083\n",
      "[8000]\tvalid_0's l1: 1.00216\tvalid_0's l2: 2.09192\n",
      "[8500]\tvalid_0's l1: 0.99174\tvalid_0's l2: 2.0637\n",
      "[9000]\tvalid_0's l1: 0.98223\tvalid_0's l2: 2.03846\n",
      "[9500]\tvalid_0's l1: 0.973709\tvalid_0's l2: 2.01619\n",
      "[10000]\tvalid_0's l1: 0.965002\tvalid_0's l2: 1.99417\n",
      "[10500]\tvalid_0's l1: 0.957425\tvalid_0's l2: 1.97503\n",
      "[11000]\tvalid_0's l1: 0.950606\tvalid_0's l2: 1.95835\n",
      "[11500]\tvalid_0's l1: 0.943296\tvalid_0's l2: 1.94002\n",
      "[12000]\tvalid_0's l1: 0.93681\tvalid_0's l2: 1.92441\n",
      "[12500]\tvalid_0's l1: 0.930803\tvalid_0's l2: 1.90983\n",
      "[13000]\tvalid_0's l1: 0.925184\tvalid_0's l2: 1.89587\n",
      "[13500]\tvalid_0's l1: 0.920271\tvalid_0's l2: 1.88408\n",
      "[14000]\tvalid_0's l1: 0.915072\tvalid_0's l2: 1.87152\n",
      "[14500]\tvalid_0's l1: 0.90987\tvalid_0's l2: 1.85884\n",
      "[15000]\tvalid_0's l1: 0.905922\tvalid_0's l2: 1.8493\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\tvalid_0's l1: 0.905922\tvalid_0's l2: 1.8493\n",
      "Fold  7 rank corr : 0.838792\n",
      "Train Index: [     0      1      2 ... 305609 305610 305612] ,Val Index: [    28     37     54 ... 305606 305608 305611]\n",
      "Training until validation scores don't improve for 2500 rounds.\n",
      "[500]\tvalid_0's l1: 1.49489\tvalid_0's l2: 3.73093\n",
      "[1000]\tvalid_0's l1: 1.38423\tvalid_0's l2: 3.32269\n",
      "[1500]\tvalid_0's l1: 1.31474\tvalid_0's l2: 3.07982\n",
      "[2000]\tvalid_0's l1: 1.26602\tvalid_0's l2: 2.92001\n",
      "[2500]\tvalid_0's l1: 1.22717\tvalid_0's l2: 2.79563\n",
      "[3000]\tvalid_0's l1: 1.19266\tvalid_0's l2: 2.69051\n",
      "[3500]\tvalid_0's l1: 1.16543\tvalid_0's l2: 2.61051\n",
      "[4000]\tvalid_0's l1: 1.13996\tvalid_0's l2: 2.53493\n",
      "[4500]\tvalid_0's l1: 1.11617\tvalid_0's l2: 2.46429\n",
      "[5000]\tvalid_0's l1: 1.09753\tvalid_0's l2: 2.41294\n",
      "[5500]\tvalid_0's l1: 1.08136\tvalid_0's l2: 2.36764\n",
      "[6000]\tvalid_0's l1: 1.06631\tvalid_0's l2: 2.32534\n",
      "[6500]\tvalid_0's l1: 1.05111\tvalid_0's l2: 2.28269\n",
      "[7000]\tvalid_0's l1: 1.03694\tvalid_0's l2: 2.24278\n",
      "[7500]\tvalid_0's l1: 1.02448\tvalid_0's l2: 2.20718\n",
      "[8000]\tvalid_0's l1: 1.01268\tvalid_0's l2: 2.17404\n",
      "[8500]\tvalid_0's l1: 1.00252\tvalid_0's l2: 2.14561\n",
      "[9000]\tvalid_0's l1: 0.993405\tvalid_0's l2: 2.12017\n",
      "[9500]\tvalid_0's l1: 0.985523\tvalid_0's l2: 2.09892\n",
      "[10000]\tvalid_0's l1: 0.976517\tvalid_0's l2: 2.07491\n",
      "[10500]\tvalid_0's l1: 0.968479\tvalid_0's l2: 2.05345\n",
      "[11000]\tvalid_0's l1: 0.962028\tvalid_0's l2: 2.03666\n",
      "[11500]\tvalid_0's l1: 0.955239\tvalid_0's l2: 2.01916\n",
      "[12000]\tvalid_0's l1: 0.948076\tvalid_0's l2: 2.00117\n",
      "[12500]\tvalid_0's l1: 0.941924\tvalid_0's l2: 1.98597\n",
      "[13000]\tvalid_0's l1: 0.936229\tvalid_0's l2: 1.97173\n",
      "[13500]\tvalid_0's l1: 0.930527\tvalid_0's l2: 1.95701\n",
      "[14000]\tvalid_0's l1: 0.925381\tvalid_0's l2: 1.94419\n",
      "[14500]\tvalid_0's l1: 0.919823\tvalid_0's l2: 1.92984\n",
      "[15000]\tvalid_0's l1: 0.915435\tvalid_0's l2: 1.91844\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\tvalid_0's l1: 0.915435\tvalid_0's l2: 1.91844\n",
      "Fold  8 rank corr : 0.833985\n",
      "Train Index: [     0      1      3 ... 305610 305611 305612] ,Val Index: [     2     12     24 ... 305586 305587 305593]\n",
      "Training until validation scores don't improve for 2500 rounds.\n",
      "[500]\tvalid_0's l1: 1.4828\tvalid_0's l2: 3.6692\n",
      "[1000]\tvalid_0's l1: 1.37158\tvalid_0's l2: 3.25846\n",
      "[1500]\tvalid_0's l1: 1.30803\tvalid_0's l2: 3.0393\n",
      "[2000]\tvalid_0's l1: 1.2555\tvalid_0's l2: 2.8678\n",
      "[2500]\tvalid_0's l1: 1.21446\tvalid_0's l2: 2.7388\n",
      "[3000]\tvalid_0's l1: 1.18005\tvalid_0's l2: 2.63333\n",
      "[3500]\tvalid_0's l1: 1.151\tvalid_0's l2: 2.54626\n",
      "[4000]\tvalid_0's l1: 1.12406\tvalid_0's l2: 2.46764\n",
      "[4500]\tvalid_0's l1: 1.10402\tvalid_0's l2: 2.40896\n",
      "[5000]\tvalid_0's l1: 1.08659\tvalid_0's l2: 2.35861\n",
      "[5500]\tvalid_0's l1: 1.07197\tvalid_0's l2: 2.31674\n",
      "[6000]\tvalid_0's l1: 1.05771\tvalid_0's l2: 2.27621\n",
      "[6500]\tvalid_0's l1: 1.04365\tvalid_0's l2: 2.23708\n",
      "[7000]\tvalid_0's l1: 1.0309\tvalid_0's l2: 2.20262\n",
      "[7500]\tvalid_0's l1: 1.01973\tvalid_0's l2: 2.17332\n",
      "[8000]\tvalid_0's l1: 1.00895\tvalid_0's l2: 2.14482\n",
      "[8500]\tvalid_0's l1: 0.998973\tvalid_0's l2: 2.11941\n",
      "[9000]\tvalid_0's l1: 0.98928\tvalid_0's l2: 2.09427\n",
      "[9500]\tvalid_0's l1: 0.980548\tvalid_0's l2: 2.07205\n",
      "[10000]\tvalid_0's l1: 0.971384\tvalid_0's l2: 2.04811\n",
      "[10500]\tvalid_0's l1: 0.963855\tvalid_0's l2: 2.02918\n",
      "[11000]\tvalid_0's l1: 0.955839\tvalid_0's l2: 2.00839\n",
      "[11500]\tvalid_0's l1: 0.948081\tvalid_0's l2: 1.98853\n",
      "[12000]\tvalid_0's l1: 0.941835\tvalid_0's l2: 1.97244\n",
      "[12500]\tvalid_0's l1: 0.936609\tvalid_0's l2: 1.95942\n",
      "[13000]\tvalid_0's l1: 0.930911\tvalid_0's l2: 1.94494\n",
      "[13500]\tvalid_0's l1: 0.924685\tvalid_0's l2: 1.9294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14000]\tvalid_0's l1: 0.918821\tvalid_0's l2: 1.91481\n",
      "[14500]\tvalid_0's l1: 0.913568\tvalid_0's l2: 1.90226\n",
      "[15000]\tvalid_0's l1: 0.908873\tvalid_0's l2: 1.89066\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\tvalid_0's l1: 0.908873\tvalid_0's l2: 1.89066\n",
      "Fold  9 rank corr : 0.838454\n",
      "Train Index: [     0      1      2 ... 305610 305611 305612] ,Val Index: [     3      9     10 ... 305591 305595 305598]\n",
      "Training until validation scores don't improve for 2500 rounds.\n",
      "[500]\tvalid_0's l1: 1.48093\tvalid_0's l2: 3.6523\n",
      "[1000]\tvalid_0's l1: 1.37155\tvalid_0's l2: 3.24436\n",
      "[1500]\tvalid_0's l1: 1.30806\tvalid_0's l2: 3.02152\n",
      "[2000]\tvalid_0's l1: 1.25691\tvalid_0's l2: 2.85283\n",
      "[2500]\tvalid_0's l1: 1.21789\tvalid_0's l2: 2.72925\n",
      "[3000]\tvalid_0's l1: 1.1838\tvalid_0's l2: 2.62806\n",
      "[3500]\tvalid_0's l1: 1.15712\tvalid_0's l2: 2.54899\n",
      "[4000]\tvalid_0's l1: 1.13032\tvalid_0's l2: 2.4699\n",
      "[4500]\tvalid_0's l1: 1.10876\tvalid_0's l2: 2.40649\n",
      "[5000]\tvalid_0's l1: 1.09038\tvalid_0's l2: 2.35348\n",
      "[5500]\tvalid_0's l1: 1.07464\tvalid_0's l2: 2.30724\n",
      "[6000]\tvalid_0's l1: 1.0602\tvalid_0's l2: 2.26649\n",
      "[6500]\tvalid_0's l1: 1.0462\tvalid_0's l2: 2.22771\n",
      "[7000]\tvalid_0's l1: 1.03268\tvalid_0's l2: 2.19139\n",
      "[7500]\tvalid_0's l1: 1.02167\tvalid_0's l2: 2.1628\n",
      "[8000]\tvalid_0's l1: 1.01003\tvalid_0's l2: 2.1311\n",
      "[8500]\tvalid_0's l1: 0.999256\tvalid_0's l2: 2.10234\n",
      "[9000]\tvalid_0's l1: 0.989872\tvalid_0's l2: 2.07659\n",
      "[9500]\tvalid_0's l1: 0.981117\tvalid_0's l2: 2.0538\n",
      "[10000]\tvalid_0's l1: 0.972492\tvalid_0's l2: 2.03204\n",
      "[10500]\tvalid_0's l1: 0.964247\tvalid_0's l2: 2.01087\n",
      "[11000]\tvalid_0's l1: 0.957047\tvalid_0's l2: 1.99257\n",
      "[11500]\tvalid_0's l1: 0.95054\tvalid_0's l2: 1.97611\n",
      "[12000]\tvalid_0's l1: 0.94462\tvalid_0's l2: 1.96081\n",
      "[12500]\tvalid_0's l1: 0.938196\tvalid_0's l2: 1.94469\n",
      "[13000]\tvalid_0's l1: 0.931674\tvalid_0's l2: 1.92804\n",
      "[13500]\tvalid_0's l1: 0.926478\tvalid_0's l2: 1.91528\n",
      "[14000]\tvalid_0's l1: 0.921352\tvalid_0's l2: 1.90294\n",
      "[14500]\tvalid_0's l1: 0.915551\tvalid_0's l2: 1.88839\n",
      "[15000]\tvalid_0's l1: 0.910584\tvalid_0's l2: 1.87667\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[15000]\tvalid_0's l1: 0.910584\tvalid_0's l2: 1.87667\n",
      "Fold 10 rank corr : 0.835256\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits= 10, shuffle=True, random_state=666)\n",
    "\n",
    "X = df_all.values\n",
    "Y = df_label.values\n",
    "Y = np.reshape(Y,[-1])\n",
    "Y2=np.asarray(Y,np.int)\n",
    "\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "# sub_preds = np.zeros(rx.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, Y2)):\n",
    "\n",
    "    train_x, train_y = X[train_idx,:], Y[train_idx]\n",
    "    valid_x, valid_y = X[valid_idx,:], Y[valid_idx]\n",
    "\n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    params = {\n",
    "    'nthread': 32, 'boosting_type': 'dart','objective': 'regression', 'metric': ['mae','mse',stats.spearmanr], \n",
    "        'learning_rate': 0.01, 'num_leaves': 70,\n",
    "    'max_depth': 9, 'subsample': 0.8, 'feature_fraction': 0.9, \n",
    "        'min_split_gain': 0.09, 'min_child_weight': 9.5,\n",
    "#         'device_type':'gpu',\n",
    "    # parameters for dart\n",
    "    'drop_rate':0.5, 'skip_drop':0.5, 'max_drop':6, 'uniform_drop':False, \n",
    "        'xgboost_dart_mode':True, 'drop_seed':1 }\n",
    "    \n",
    "\n",
    "    if n_fold >= 0:\n",
    "        dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "        dval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain)\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=15000, valid_sets=[dval], early_stopping_rounds=2500, verbose_eval=500)\n",
    "\n",
    "\n",
    "        tmp_valid = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "        oof_preds[valid_idx] = tmp_valid\n",
    "      \n",
    "        # Make the feature importance dataframe\n",
    "        gain = bst.feature_importance('gain')\n",
    "        fold_importance_df = pd.DataFrame({'feature':bst.feature_name(),'split':bst.feature_importance('split'),\n",
    "                                           'gain':100*gain/gain.sum(),  'fold':n_fold,}).sort_values('gain',ascending=False)\n",
    "\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d rank corr : %.6f' % (n_fold + 1, stats.spearmanr(valid_y, oof_preds[valid_idx])[0]))\n",
    "\n",
    "        del bst, train_x, train_y, valid_x, valid_y\n",
    "        \n",
    "\n",
    "\n",
    "# app_test = pd.read_csv('testing-set.csv', usecols=['order_id'])\n",
    "# predF=pd.DataFrame({\"order_id\":rid, \"deal_or_not\":sub_preds})\n",
    "# preds = pd.DataFrame({\"order_id\":app_test['order_id']})\n",
    "# preds=preds.merge(predF, on=\"order_id\",how=\"outer\")\n",
    "# preds['deal_or_not'] = preds.deal_or_not.apply(lambda x: nanMapping(x))\n",
    "# preds.to_csv(\"output/lgb_dart_\" + str(roc_auc_score(Y, oof_preds)) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
