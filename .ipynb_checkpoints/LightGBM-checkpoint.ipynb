{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pickle\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import time, datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import gc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import jieba, pdb\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "# load stopwords set\n",
    "stopword_set = set()\n",
    "with open('jieba_dict/stopwords.txt','r', encoding='utf-8') as stopwords:\n",
    "    for stopword in stopwords:\n",
    "        stopword_set.add(stopword.strip('\\n'))\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"word2vec2.model\")\n",
    "\n",
    "\n",
    "def create_dictionaries(p_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(p_model.wv.vocab.keys(), allow_update=True)\n",
    "    w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2indx.keys()}  # 词语的词向量\n",
    "    return w2indx, w2vec\n",
    "\n",
    "def Convert_uniqueid(x, sets):\n",
    "    return np.where(sets==x)[0][0]\n",
    "\n",
    "def parsePathalias(x):\n",
    "    if x == 'None':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(x)\n",
    "\n",
    "def Convert_Date(x):\n",
    "    timestamp = x\n",
    "    timeArray = time.localtime(timestamp)\n",
    "    datetime = time.strftime(\"%Y-%m-%d %H:%M:%S\", timeArray)\n",
    "    return datetime\n",
    "\n",
    "def Date2Ticks(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = str(Year+'/'+Month+'/'+Day)\n",
    "    return time.mktime(datetime.datetime.strptime(date1, \"%Y/%m/%d\").timetuple())\n",
    "\n",
    "index_dict, word_vectors= create_dictionaries(model)\n",
    "output = open(\"wordwmbedding.pkl\", 'wb')\n",
    "pickle.dump(index_dict, output)  # 索引字典\n",
    "pickle.dump(word_vectors, output)  # 词向量字典\n",
    "output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4359fa65cc16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mdf_tag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf_tag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0mdf_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_cat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_add\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_ts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_tags\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Uid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# df_airline = df_airline.drop_duplicates(subset='group_id', keep='first', inplace=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_tags' is not defined"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df_tag = pd.read_csv(\"train/train_tags2.txt\", delimiter=\",\",encoding = \"utf-8\")\n",
    "df_add = pd.read_csv(\"train/train_additional.txt\",delimiter=\" \")\n",
    "df_cat = pd.read_csv(\"train/train_category.txt\",delimiter=\" \")\n",
    "df_ts = pd.read_csv(\"train/train_temporalspatial.txt\",delimiter=\" \")\n",
    "df_label = pd.read_csv(\"train/train_label.txt\",delimiter=\" \")\n",
    "\n",
    "'''\n",
    "Cat: Uid Pid Category Subcategory Concept\n",
    "TAG: Uid Pid Title Mediatype Alltags\n",
    "TS:  Uid Pid Postdate Latitude Longitude Geoaccuracy\n",
    "Add: Uid Pid Pathalias Ispublic Mediastatus\n",
    "\n",
    "'''\n",
    "# date Conversion\n",
    "uidset =      np.unique(np.asarray(df_cat['Uid'].values.tolist()))\n",
    "pidset =      np.unique(np.asarray(df_cat['Pid'].values.tolist()))\n",
    "Category =    np.unique(np.asarray(df_cat['Category'].values.tolist()))\n",
    "Subcategory = np.unique(np.asarray(df_cat['Subcategory'].values.tolist()))\n",
    "Concept =     np.unique(np.asarray(df_cat['Concept'].values.tolist()))\n",
    "\n",
    "\n",
    "month = {'Jan': '01', 'Feb': '02' , 'Mar':'03' ,'Apr': '04', \n",
    "'May': '05', 'Jun': '06' , 'Jul': '07' , 'Aug':'08', \n",
    "'Sep':'09', 'Oct':'10' , 'Nov':'11', 'Dec':'12' }\n",
    "\n",
    "# group data\n",
    "df_cat['Uid']=df_cat.Uid.apply(lambda x: Convert_uniqueid(x, uidset))\n",
    "df_cat['Pid']=df_cat.Pid.apply(lambda x: Convert_uniqueid(x, pidset))\n",
    "df_cat['Category']= df_cat.Category.apply(lambda x: Convert_uniqueid(x, Category))\n",
    "df_cat['Subcategory']= df_cat.Subcategory.apply(lambda x:  Convert_uniqueid(x, Subcategory))\n",
    "df_cat['Concept']= df_cat.Concept.apply(lambda x: Convert_uniqueid(x, Concept))\n",
    "\n",
    "df_add['Uid']=df_add.Uid.apply(lambda x: Convert_uniqueid(x, uidset))\n",
    "df_add['Pid']=df_add.Pid.apply(lambda x: Convert_uniqueid(x, pidset))\n",
    "df_add['Pathalias']=df_add.Pathalias.apply(lambda x: parsePathalias(x))\n",
    "df_add['Ispublic']=df_add.Ispublic.apply(lambda x: int(x))\n",
    "df_add['Mediastatus']=df_add.Mediastatus.apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "df_ts['Uid']=df_ts.Uid.apply(lambda x: Convert_uniqueid(x, uidset))\n",
    "df_ts['Pid']=df_ts.Pid.apply(lambda x: Convert_uniqueid(x, pidset))\n",
    "df_ts['Postdate']=df_ts.Pid.apply(lambda x: int(x))\n",
    "df_ts['Latitude']=df_ts.Pid.apply(lambda x: float(x))\n",
    "df_ts['Longitude']=df_ts.Pid.apply(lambda x: float(x))\n",
    "df_ts['Geoaccuracy']=df_ts.Pid.apply(lambda x: float(x))\n",
    "\n",
    "df_tag['Uid']=df_tag.Uid.apply(lambda x: Convert_uniqueid(x, uidset))\n",
    "df_tag['Pid']=df_tag.Pid.apply(lambda x: Convert_uniqueid(x, pidset))\n",
    "df_tag['Title']= df_tag.Title.apply(lambda x: len(x))\n",
    "\n",
    "df_label['score'] = df_label.score.apply(lambda x: int(x))\n",
    "\n",
    "# df_airline = df_airline.drop_duplicates(subset='group_id', keep='first', inplace=False)\n",
    "\n",
    "# group_used_cols=['group_id','Begin_Date','Begin_Tick','days','Area','SubLine','price', 'product_name']\n",
    "# df_group_0 = df_group[group_used_cols].merge(df_airline, on='group_id')\n",
    "\n",
    "# # for order data\n",
    "# df_order_1['Order_Date']=df_order_1.order_date.apply(lambda x: Convert_Date(x))\n",
    "# df_order_1['Order_Tick']=df_order_1.order_date.apply(lambda x: Date2Ticks(x))\n",
    "# df_order_1['order_id']=df_order_1.order_id.apply(lambda x: Convert_orderid(x))\n",
    "# df_order_1['Source_1']= df_order_1.source_1.apply(lambda x: int(x[11:]))\n",
    "# df_order_1['Source_2']= df_order_1.source_2.apply(lambda x: int(x[11:]))\n",
    "# df_order_1['Unit']= df_order_1.unit.apply(lambda x: int(x[11:]))\n",
    "# df_order_1['Begin_Date']=pd.to_datetime(df_order_1['Begin_Date'])\n",
    "# df_order_1['Order_Date']=pd.to_datetime(df_order_1['Order_Date'])\n",
    "# df_order_1['PreDays']=(df_order_1['Begin_Date']-df_order_1['Order_Date']).dt.days\n",
    "# df_order_1['Begin_Date_Weekday']= df_order_1['Begin_Date'].dt.dayofweek\n",
    "# df_order_1['Order_Date_Weekday']= df_order_1['Order_Date'].dt.dayofweek\n",
    "# df_order_1['Return_Date_Weekday']= (df_order_1['Begin_Date'].dt.dayofweek+df_order_1['days'])%7\n",
    "# df_order_1['tick_diff'] = (df_order_1['Begin_Tick'] - df_order_1['Order_Tick'])/10000\n",
    "# df_order_1['price'] = df_order_1['price']/1000\n",
    "\n",
    "# order_used_columns=['order_id', 'group_id','tick_diff', 'Source_1', 'Source_2', 'Unit',\n",
    "# 'people_amount', 'Begin_Tick','days', 'Order_Tick', 'Area', 'SubLine', 'price','PreDays','Begin_Date_Weekday', \n",
    "# 'Order_Date_Weekday', 'Return_Date_Weekday', 'fly_t', 'fly_date',\n",
    "# \"src_airport\", \"arrive_t\", \"arrive_date\", \"dst_airport\", 'product_name']\n",
    "\n",
    "# df_order_2=df_order_1[order_used_columns].merge(df_day_schedule[['group_id','title']], on='group_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 305613 training with 305612 labels\n",
      "        11.18\n",
      "0       15.15\n",
      "1       10.99\n",
      "2        8.63\n",
      "3       11.16\n",
      "4       12.74\n",
      "5       11.46\n",
      "6       10.62\n",
      "7       14.24\n",
      "8        6.51\n",
      "9        5.00\n",
      "10      13.62\n",
      "11       9.67\n",
      "12       8.85\n",
      "13       8.67\n",
      "14       8.61\n",
      "15       9.00\n",
      "16       8.96\n",
      "17       8.52\n",
      "18       9.37\n",
      "19       7.93\n",
      "20       9.04\n",
      "21       8.66\n",
      "22       8.39\n",
      "23       8.52\n",
      "24       8.79\n",
      "25       9.29\n",
      "26       9.10\n",
      "27       9.18\n",
      "28       9.63\n",
      "29      10.14\n",
      "...       ...\n",
      "305582   5.13\n",
      "305583   8.57\n",
      "305584   3.81\n",
      "305585   1.00\n",
      "305586   9.55\n",
      "305587   5.98\n",
      "305588   7.09\n",
      "305589   1.00\n",
      "305590   6.63\n",
      "305591   1.00\n",
      "305592   3.00\n",
      "305593   1.00\n",
      "305594   4.91\n",
      "305595   1.00\n",
      "305596   1.00\n",
      "305597   1.00\n",
      "305598   2.32\n",
      "305599   2.58\n",
      "305600   3.32\n",
      "305601   4.46\n",
      "305602   3.32\n",
      "305603   3.70\n",
      "305604   1.00\n",
      "305605   5.17\n",
      "305606   6.32\n",
      "305607   4.00\n",
      "305608   1.00\n",
      "305609   2.00\n",
      "305610   6.89\n",
      "305611   1.58\n",
      "\n",
      "[305612 rows x 1 columns]\n",
      "Got 305613 training with 305612 labels\n"
     ]
    }
   ],
   "source": [
    "print(\"Got %d training with %d labels\" % (len(df_cat), len(df_label)))\n",
    "\n",
    "\n",
    "df_all = df_cat.merge(df_tag, on=['Uid', 'Pid'])\n",
    "df_all2 = df_ts.merge(df_all, on=['Uid', 'Pid'])\n",
    "df_all3 = df_tag.merge(df_all2, on=['Uid', 'Pid'])\n",
    "print(df_label)\n",
    "# train/test data\n",
    "print(\"Got %d training with %d labels\" % (len(df_all), len(df_label)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test data\n",
    "# df_train_1=df_train.merge(df_order_2,on='order_id')\n",
    "# df_result_1=df_result.merge(df_order_2,on='order_id')\n",
    "# df_train_1.to_csv('myTrain2.csv', index=False)\n",
    "# df_result_1.to_csv('myTest2.csv', index=False)\n",
    "\n",
    "# Y=df_train_1['deal_or_not'].values.tolist()\n",
    "# swX_tmp = (df_train_1['product_name']).values.tolist()\n",
    "# Xid = df_train_1['order_id'].values.tolist()\n",
    "# del df_train_1['deal_or_not'] \n",
    "# del df_train_1['product_name']\n",
    "# del df_train_1['group_id'] \n",
    "# del df_train_1['order_id']\n",
    "# X = df_train_1.values.tolist()\n",
    "\n",
    "# rid = df_result_1['order_id'].values.tolist()\n",
    "# swrx = (df_result_1['product_name']).values.tolist()\n",
    "# del df_result_1['product_name']\n",
    "# del df_result_1['deal_or_not']\n",
    "# del df_result_1['order_id']\n",
    "# del df_result_1['group_id']\n",
    "\n",
    "# rx = df_result_1.values.tolist()\n",
    "\n",
    "\n",
    "# sX, sY, Xid =np.asarray(X), np.asarray(Y), np.asarray(Xid)\n",
    "# rx,rid = np.asarray(rx), np.asarray(rid)\n",
    "# X,Y, swX=[],[], []\n",
    "# for i in range(len(sY)):\n",
    "#    # if (int(Xid[i])<=204000):\n",
    "#         X.append(sX[i,:])\n",
    "#         Y.append(sY[i])\n",
    "#         swX.append(swX_tmp[i])\n",
    "# X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "# def text_to_index_array(p_new_dic, p_sen):  # 文本转为索引数字模式\n",
    "#     new_sentences = []\n",
    "#     for sen in p_sen:\n",
    "#         new_sen = []\n",
    "#         for word in str(sen):\n",
    "#             try:\n",
    "#                 new_sen.append(p_new_dic[word])  # 单词转索引数字\n",
    "#             except:\n",
    "#                 new_sen.append(0)  # 索引字典里没有的词转为数字0\n",
    "#         new_sentences.append(new_sen)\n",
    "\n",
    "#     return np.array(new_sentences)\n",
    "\n",
    "\n",
    "# wX = text_to_index_array(index_dict, swX)\n",
    "# wrx = text_to_index_array(index_dict, swrx)\n",
    "# wX = sequence.pad_sequences(wX, maxlen=60)\n",
    "# wrx = sequence.pad_sequences(wrx, maxlen=60)\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "# pt_X = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "# sc_y = StandardScaler()\n",
    "# sc_X = StandardScaler()\n",
    "# X = sc_X.fit_transform(X)\n",
    "# rx = sc_X.transform(rx)\n",
    "\n",
    "# # X=np.concatenate([X, wX], axis=1)\n",
    "# # rx=np.concatenate([rx, wrx], axis=1)\n",
    "# # xlen=len(X)\n",
    "# # from sklearn.preprocessing import normalize\n",
    "# # Xtmp=normalize(np.concatenate([X, rx], axis=0),norm='max', axis=0)\n",
    "# # X=Xtmp[:xlen]\n",
    "# # rx=Xtmp[xlen:]\n",
    "\n",
    "# print(X.shape)\n",
    "\n",
    "# # np.save(\"data.npy\", [X,Y,rx])\n",
    "# # [X,Y,rx] = np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [305613, 305612]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-41f42d14ae9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mn_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0mset\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthat\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \"\"\"\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 230\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [305613, 305612]"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits= 10, shuffle=True, random_state=666)\n",
    "\n",
    "X = df_all.values\n",
    "Y = df_label.values\n",
    "\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "# sub_preds = np.zeros(rx.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, Y)):\n",
    "\n",
    "    train_x, train_y,train_id = X[train_idx,:], Y[train_idx], Xid[train_idx]\n",
    "    valid_x, valid_y = X[valid_idx,:], Y[valid_idx]\n",
    "    valid_id=Xid[valid_idx]\n",
    "\n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    params = {\n",
    "    'nthread': 8, 'boosting_type': 'dart','objective': 'regression', 'metric': 'auc', \n",
    "        'learning_rate': 0.01, 'num_leaves':70,\n",
    "    'max_depth': 9, 'subsample': 0.8, 'feature_fraction': 0.9, \n",
    "        'min_split_gain': 0.09, 'min_child_weight': 9.5,\n",
    "        #'device_type':'gpu',\n",
    "    # parameters for dart\n",
    "    'drop_rate':0.5, 'skip_drop':0.5, 'max_drop':6, 'uniform_drop':False, \n",
    "        'xgboost_dart_mode':True, 'drop_seed':1 }\n",
    "    \n",
    "\n",
    "\n",
    "    if n_fold >= 0:\n",
    "        dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "        dval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain)\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=15000, valid_sets=[dval], early_stopping_rounds=2500, verbose_eval=500)\n",
    "\n",
    "\n",
    "        tmp_valid = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "        tmp_valid[tmp_valid>0.8]=1\n",
    "        tmp_valid[tmp_valid<0.2]=0\n",
    "        oof_preds[valid_idx] = tmp_valid\n",
    "        sub_preds += bst.predict(rx, num_iteration=bst.best_iteration) / folds.n_splits\n",
    "        \n",
    "\n",
    "        # Make the feature importance dataframe\n",
    "        gain = bst.feature_importance('gain')\n",
    "        fold_importance_df = pd.DataFrame({'feature':bst.feature_name(),'split':bst.feature_importance('split'),\n",
    "                                           'gain':100*gain/gain.sum(),  'fold':n_fold,}).sort_values('gain',ascending=False)\n",
    "\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "\n",
    "        del bst, train_x, train_y, valid_x, valid_y\n",
    "        \n",
    "\n",
    "\n",
    "# app_test = pd.read_csv('testing-set.csv', usecols=['order_id'])\n",
    "# predF=pd.DataFrame({\"order_id\":rid, \"deal_or_not\":sub_preds})\n",
    "# preds = pd.DataFrame({\"order_id\":app_test['order_id']})\n",
    "# preds=preds.merge(predF, on=\"order_id\",how=\"outer\")\n",
    "# preds['deal_or_not'] = preds.deal_or_not.apply(lambda x: nanMapping(x))\n",
    "# preds.to_csv(\"output/lgb_dart_\" + str(roc_auc_score(Y, oof_preds)) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
